{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Quality","text":"<p>Welcome! This package is designed as a simple and fast way for engineering teams with billions of records, as well as those of you working in Jupyter Notebooks, to quickly and repeatably measure the quality of your data.</p> <p>If we record data quality the same way, it's easier to share results with each other and we can re-use existing resources like dashboards.</p>"},{"location":"#our-motiviation","title":"Our motiviation","text":"<p>We wanted a data quality package that fulfilled these aims:</p> <ul> <li>Open-Source &amp; Permissive: Licensed under Apache 2.0, with no commercial strings attached - opensource forever.</li> <li>Simplicity First:  Teams can get started fast \u2014 plug in your DataFrame, define rules, and get insights with minimal code. What you do in a jupyter notebook is the same code as in production at scale. How you schedule, sample, alert and visualise is up to you (and out of scope in this package)</li> <li>Handles Nested Data: Supports Spark DataFrames with nested data.</li> <li>Comparisons between values: Able to compare values with a range of logical operators between different columns, especially involving time of events.</li> <li>Built for Insight: Outputs a standard tabular format, designed to give useful insights and be easy to visualise in a dashboard. You should compare results from different rules on the same field to help diagnose data quality errors, this is more insightful than the number of rules that passed or failed some threshold.</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We are grateful for DAMA-UK (Data Management Association, UK Chapter) for granting us permission to reference and use their Data Quality Dimensions throughout the tutorials and code. Source: Dama International. 2017. DAMA-DMBOK: Data Management Body of Knowledge (2nd Edition). Technics Publications, LLC, Denville, NJ, USA.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#rules","title":"Rules","text":""},{"location":"api/#gchq_data_quality.rules.uniqueness.UniquenessRule","title":"<code>gchq_data_quality.rules.uniqueness.UniquenessRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule for assessing uniqueness in a column.</p> <p>Measures the proportion of unique, non-null values in a specified column. This is useful for checking distinct identifiers or reference keys. Additional null-like values can be specified via <code>na_values</code>.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Column to evaluate for uniqueness.</p> <code>na_values</code> <code>Any | list[Any] | None</code> <p>Values to treat as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Data quality dimension (Uniqueness).</p> <code>rule_id</code> <code>str | None</code> <p>Optional rule identifier.</p> <code>rule_description</code> <code>str | None</code> <p>Optional description for this rule.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from gchq_data_quality.rules.uniqueness import UniquenessRule\n&gt;&gt;&gt; df = pd.DataFrame({'id': [1, 2, 3, 3, None]})\n\n# Basic uniqueness check\n&gt;&gt;&gt; rule = UniquenessRule(field='id')\n&gt;&gt;&gt; result = rule.evaluate(df)\n&gt;&gt;&gt; print(result.pass_rate)\n0.75\n\n# Specify additional NA values\n&gt;&gt;&gt; rule = UniquenessRule(field='id', na_values=[-1])\n&gt;&gt;&gt; df = pd.DataFrame({'id': [1, 2, -1, 3, 3]})\n&gt;&gt;&gt; result = rule.evaluate(df)\n&gt;&gt;&gt; print(result.pass_rate)\n0.75\n</code></pre> Note <p>The pass_rate metric is calculated as (number of unique values) / (number of non-null records). Therefore, if every value in the column appears exactly twice, pass_rate will be 0.5 (not 0.0!). For columns with even more duplication, pass_rate will decrease and approach zero as the number of unique values becomes small relative to the number of total records. Only if every record is identical will pass_rate be 1 / N (where N is the number of records).</p> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the uniqueness score (<code>pass_rate</code>), identifiers for failed records,</p> <p>a sample of duplicate values, the number of records evaluated, and rule metadata.</p> <p>See DataQualityResult documentation for further attribute details.</p>"},{"location":"api/#gchq_data_quality.rules.completeness.CompletenessRule","title":"<code>gchq_data_quality.rules.completeness.CompletenessRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule to calculate the completeness score for a field.</p> <p>Completeness is measured as the proportion of non-null values in the specified column. Values specified in <code>na_values</code> are converted to nulls prior to calculation.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>The column name to assess.</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Additional indicators to treat as missing.</p> <code>rule_id</code> <code>str | None</code> <p>Optional identifier for the rule.</p> <code>rule_description</code> <code>str | None</code> <p>Optional description of the rule.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates completeness for the chosen fields on a Pandas or Spark DataFrame. Returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = CompletenessRule(field=\"column_name\")\n&gt;&gt;&gt; result = rule.evaluate(df)\n&gt;&gt;&gt; print(result.pass_rate)\n\n&gt;&gt;&gt; rule = CompletenessRule(field=\"column_name\", na_values=\"missing\")\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains completeness score (<code>pass_rate</code>), field name,</p> <p>number of records evaluated, and rule metadata. See DataQualityResult documentation</p> <p>for further attribute details.</p>"},{"location":"api/#gchq_data_quality.rules.accuracy.AccuracyRule","title":"<code>gchq_data_quality.rules.accuracy.AccuracyRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule to check if values meet a list of valid (or invalid) values.</p> <p>Skips NULLs, including those recognised via <code>na_values</code>. Instantiate this class and call <code>.evaluate(df)</code> to assess data quality for the chosen column.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>The column to check for accuracy.</p> <code>valid_values</code> <code>list[Any]</code> <p>The set of acceptable values for the field.</p> <code>inverse</code> <code>bool</code> <p>If True, values in <code>valid_values</code> are considered invalid (exclusion list).</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Additional indicators to treat as missing values.</p> <code>rule_id</code> <code>str | None</code> <p>Optional identifier for the rule.</p> <code>rule_description</code> <code>str | None</code> <p>Optional description of the rule.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = AccuracyRule(field=\"category\", valid_values=[\"A\", \"B\", \"C\"])\n&gt;&gt;&gt; result = rule.evaluate(df)\n&gt;&gt;&gt; print(result.pass_rate)\n&gt;&gt;&gt; print(result.records_failed_ids)\n\n&gt;&gt;&gt; rule = AccuracyRule(\n...     field=\"department\",\n...     valid_values=[\"HR\", \"IT\", \"Sales\"],\n...     na_values=[\"N/A\", \"N/K\"]\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n&gt;&gt;&gt; rule = AccuracyRule(\n...     field=\"status\",\n...     valid_values=[\"expired\", \"deleted\"],\n...     inverse=True # value must NOT be expired or deleted\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>An object containing the accuracy score (pass_rate),</p> <p>the indices of failed rows (records_failed_ids), a sample of failed values</p> <p>(records_failed_sample), the number of records evaluated, and further rule metadata.</p> <p>See DataQualityResult documentation for full details.</p>"},{"location":"api/#gchq_data_quality.rules.consistency.ConsistencyRule","title":"<code>gchq_data_quality.rules.consistency.ConsistencyRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule for evaluating data consistency based on boolean expressions (with an optional condition).</p> <p>Expressions may use any valid Pandas eval syntax that returns a boolean result. Backticks are required around all column names. Nulls and additional na_values are handled according to the skip policy.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>The column to check for consistency.</p> <code>expression</code> <code>str | dict[str, str]</code> <p>A boolean expression, or a conditional {'if', 'then'} dictionary (with backticks for column names).</p> <code>skip_if_null</code> <code>Literal['all', 'any', 'never']</code> <p>Controls row skipping for null values in relevant columns.</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Additional values considered as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Associated data quality dimension - you may want to override it in this rule.</p> <code>rule_id</code> <code>str | None</code> <p>Optional identifier for the rule.</p> <code>rule_description</code> <code>str | None</code> <p>Optional description of the rule.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"score\",\n...     expression=\"`score` &gt;= 50\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"completion_date\",\n...     expression={\"if\": \"`status` == 'completed'\", \"then\": \"`completion_date`.notnull()\"},\n        data_quality_dimension='Completeness' # you can override the DAMA Dimension\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# all series .str. methods are available\n&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"postcode\",\n...     expression={\n...         \"if\": \"`country` == 'UK'\",\n...         \"then\": \"`postcode`.str.match(r'^[A-Z]{2}[0-9]{2}$')\"\n...     }\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Date parts and arithmetic using .dt accessor\n&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"report_year\",\n...     expression=\"`report_date`.dt.year == `report_year`\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Boolean logic (AND, OR, NOT) with grouping and comparisons\n&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"flag\",\n...     expression=\"(`score` &gt; 90) &amp; ((`status` == 'active') | ~`is_archived`)\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Using mathematical operations\n&gt;&gt;&gt; rule = ConsistencyRule(\n...     field=\"predicted\",\n...     expression=\"abs(`actual` - `predicted`) &lt; 10\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>An object containing the consistency score (<code>pass_rate</code>),</p> <p>number of records evaluated, a sample of inconsistent records, and details of failed row indices.</p> <p>See DataQualityResult documentation for full attribute descriptions.</p>"},{"location":"api/#gchq_data_quality.rules.timeliness.TimelinessRelativeRule","title":"<code>gchq_data_quality.rules.timeliness.TimelinessRelativeRule</code>","text":"<p>               Bases: <code>TimelinessBaseRule</code></p> <p>Rule to assess whether datetime values fall between relative time boundaries from a reference date (which can be a static value or come from a column in the data source).</p> <p>Timedelta bounds are specified for start and end, relative to a reference date or reference column. All datetime comparisons are performed in UTC, with date-only values assumed midnight. Only one of <code>reference_date</code> or <code>reference_column</code> may be provided. If neither is given, current UTC time is used as the reference_date.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Name of the datetime column to assess.</p> <code>start_timedelta</code> <code>timedelta | str | int | float | None</code> <p>Lower offset from the reference.</p> <code>end_timedelta</code> <code>timedelta | str | int | float | None</code> <p>Upper offset from the reference.</p> <code>reference_date</code> <code>str | datetime | Timestamp | None</code> <p>Fixed reference date/time (UTC).</p> <code>reference_column</code> <code>str | None</code> <p>Per-row column providing reference dates/times.</p> <code>dayfirst</code> <code>bool</code> <p>If True, parses ALL dates as day/month/year.</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Values treated as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Associated data quality dimension.</p> <code>rule_id</code> <code>str | None</code> <p>Optional rule identifier.</p> <code>rule_description</code> <code>str | None</code> <p>Optional rule description.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Note <p>Integer values into start or end timedelta are assumed to be nanoseconds (default pandas.to_timedelta() behaviour)</p> Example <pre><code>&gt;&gt;&gt; rule = TimelinessRelativeRule(\n...     field=\"event_date\",\n...     start_timedelta=\"0d\",\n...     end_timedelta=\"30d\",\n...     reference_date=\"2024-01-01T00:00:00Z\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n&gt;&gt;&gt; rule = TimelinessRelativeRule(\n...     field=\"booking_date\",\n...     start_timedelta=\"-1d\",\n...     end_timedelta=\"5d\",\n...     reference_column=\"event_date\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Require event dates at least 5 days after the reference date\n&gt;&gt;&gt; rule = TimelinessRelativeRule(\n...     field=\"event_date\",\n...     start_timedelta=\"5d\",\n...     end_timedelta=None,\n...     reference_date=\"2023-06-01\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; rule = TimelinessRelativeRule(\n...     field=\"sensor_timestamp\",\n...     start_timedelta=timedelta(hours=-12),\n...     end_timedelta=timedelta(hours=12)\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the timeliness score (<code>pass_rate</code>), indices and sample of failed records,</p> <p>total records evaluated, and metadata. See DataQualityResult documentation for details.</p>"},{"location":"api/#gchq_data_quality.rules.timeliness.TimelinessStaticRule","title":"<code>gchq_data_quality.rules.timeliness.TimelinessStaticRule</code>","text":"<p>               Bases: <code>TimelinessBaseRule</code></p> <p>Rule to check whether datetime values in a column fall between absolute start and end date boundaries (inclusive).</p> <p>Suitable where both boundaries are fixed or known in advance (e.g., events occurring in January 2024). All dates are treated as, or coerced to, UTC, with date-only strings assumed to be midnight. Invalid or unparsable datetime values are treated as missing. Combine with a validity rule and completeness rule on the same field for the best insights.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Name of the datetime column to assess.</p> <code>start_date</code> <code>str | datetime | Timestamp | None</code> <p>Inclusive lower boundary for valid values.</p> <code>end_date</code> <code>str | datetime | Timestamp | None</code> <p>Inclusive upper boundary for valid values.</p> <code>dayfirst</code> <code>bool</code> <p>If True, parses ALL dates and rule inputs as day/month/year, otherwise month/day/year.</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Values treated as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Associated data quality dimension. (You may want to override, e.g. perhaps 'Consistency' makes sense for some of these rules)</p> <code>rule_id</code> <code>str | None</code> <p>Optional rule identifier.</p> <code>rule_description</code> <code>str | None</code> <p>Optional rule description.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = TimelinessStaticRule(\n...     field=\"event_date\",\n...     start_date=\"2024-01-01T00:00:00Z\",\n...     end_date=\"2024-01-31T23:59:59Z\"\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Only require that dates are on or after 2023-06-01\n&gt;&gt;&gt; rule = TimelinessStaticRule(\n...     field=\"date_col\",\n...     start_date=\"2023-06-01\",\n...     end_date=None\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Using string-based boundaries with day-first format\n&gt;&gt;&gt; rule = TimelinessStaticRule(\n...     field=\"timestamp\",\n...     start_date=\"01/06/2023\",\n...     end_date=\"30/06/2023\",\n...     dayfirst=True # also assumes dates in field 'timestamp' are dayfirst\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# Using Python datetime objects as boundaries\n&gt;&gt;&gt; from datetime import datetime, timezone\n&gt;&gt;&gt; rule = TimelinessStaticRule(\n...     field=\"timestamp\",\n...     start_date=datetime(2023, 6, 1, 0, 0, tzinfo=timezone.utc),\n...     end_date=datetime(2023, 6, 30, 23, 59, tzinfo=timezone.utc),\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the timeliness score (<code>pass_rate</code>), indices of failed records,</p> <p>a sample of those records, and metadata. See DataQualityResult documentation for details.</p>"},{"location":"api/#gchq_data_quality.rules.validity.ValidityNumericalRangeRule","title":"<code>gchq_data_quality.rules.validity.ValidityNumericalRangeRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule for validating numerical values against a specified range.</p> <p>Considers only non-null values; values outside the range or failing coercion to numeric are considered invalid. Diagnostic samples and record indices are returned for values outside the allowed range.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Column to check for numerical range validity.</p> <code>min_value</code> <code>float</code> <p>Minimum allowed value (inclusive; defaults to -infinity).</p> <code>max_value</code> <code>float</code> <p>Maximum allowed value (inclusive; defaults to +infinity).</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Additional values to treat as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Data quality dimension (Validity).</p> <code>rule_id</code> <code>str | None</code> <p>Optional rule identifier.</p> <code>rule_description</code> <code>str | None</code> <p>Optional rule description.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = ValidityNumericalRangeRule(\n...     field=\"age\",\n...     min_value=0,\n...     max_value=120\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# no upper limit\n&gt;&gt;&gt; rule = ValidityNumericalRangeRule(\n...     field=\"temp_c\",\n...     min_value=0,\n...     na_values=-999\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n# no lower limit\n&gt;&gt;&gt; rule = ValidityNumericalRangeRule(\n...     field=\"score\",\n...     max_value=100,\n...     na_values=['missing', 'N/A']\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the validity score (<code>pass_rate</code>),</p> <p>sample and indices of failed records, total records evaluated,</p> <p>and rule metadata. See DataQualityResult documentation for details.</p>"},{"location":"api/#gchq_data_quality.rules.validity.ValidityRegexRule","title":"<code>gchq_data_quality.rules.validity.ValidityRegexRule</code>","text":"<p>               Bases: <code>BaseRule</code></p> <p>Rule for validating string values against a regular expression.</p> <p>Considers only non-null entries, with additional missing-value patterns specified via <code>na_values</code>. A diagnostic sample of values failing the regex is returned if present.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Column to check for regex validity.</p> <code>regex_pattern</code> <code>str</code> <p>Regular expression pattern for validation.</p> <code>na_values</code> <code>str | list[Any] | None</code> <p>Additional values to treat as missing.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Data quality dimension (Validity) by default.</p> <code>rule_id</code> <code>str | None</code> <p>Optional rule identifier.</p> <code>rule_description</code> <code>str | None</code> <p>Optional rule description.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame) -&gt; DataQualityResult Evaluates the rule on the provided Pandas or Spark DataFrame and returns the metrics and diagnostics of the rule evaluation.</p> Example <pre><code>&gt;&gt;&gt; rule = ValidityRegexRule(\n...     field=\"email\",\n...     regex_pattern=r'^[^@]+@[^@]+\\.[^@]+$'\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n\n&gt;&gt;&gt; rule = ValidityRegexRule(\n...     field=\"country_code\",\n...     regex_pattern=r'^[A-Z]{2}$'\n... )\n&gt;&gt;&gt; result = rule.evaluate(df)\n</code></pre> Note <p>To centrally manage and update regex patterns you can provide a separate YAML file containing named regex patterns (e.g., EMAIL_REGEX, POSTCODE_REGEX). Keys in this file are substituted in your main configuration files wherever referenced, enabling consistent and maintainable regex use.</p> <p>When storing regex patterns in YAML, always use single quotes ('pattern') rather than double quotes to ensure correct handling of typical regex escape characters, such as \\d or \\w.</p> <pre><code># regex_patterns.yaml\nEMAIL_REGEX: '^[^@]+@[^@]+\\.[^@]+$'\nPOSTCODE_REGEX: '^[A-Z]{2}[0-9]{2,3}\\s?[0-9][A-Z]{2}$'\n\n# In your DQ config YAML, use the key in place of the regex pattern:\nrules:\n- function: validity_regex\n    field: email\n    regex_pattern: EMAIL_REGEX\n\n# Python code to load with substitution:\n&gt;&gt;&gt; from gchq_data_quality.config import DataQualityConfig\n&gt;&gt;&gt; dq_config = DataQualityConfig.from_yaml(\n...     'your_config.yaml',\n...     regex_yaml_path='regex_patterns.yaml'\n... )\n</code></pre> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the validity score (<code>pass_rate</code>),</p> <p>sample and indices of failed records, number of evaluated records,</p> <p>and rule metadata. See DataQualityResult documentation for details.</p>"},{"location":"api/#data-quality-configuration-and-results","title":"Data Quality Configuration and Results","text":""},{"location":"api/#gchq_data_quality.config.DataQualityConfig","title":"<code>gchq_data_quality.config.DataQualityConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration describing a set of data quality checks to be run on a dataset.</p> <p>Typically constructed by loading a YAML file specifying the dataset and a list of rule definitions. Can also be created programmatically.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>str | None</code> <p>Dataset name or identifier.</p> <code>measurement_sample</code> <code>str | None</code> <p>Description of data sample.</p> <code>lifecycle_stage</code> <code>str | None</code> <p>The lifecycle stage at which data is measured.</p> <code>measurement_time</code> <code>datetime | None</code> <p>Measurement timestamp.</p> <code>dataset_id</code> <code>str | int | float | None</code> <p>Local data catalogue ID.</p> <code>rules</code> <code>list[RuleType] | None</code> <p>List of rule models.</p> Example <pre><code># Loading from YAML\nconfig = DataQualityConfig.from_yaml(\"my_config.yaml\")\n-- see tutorial for how to specify the yaml file, or create a config programatically and use .to_yaml() to create something to start with.\n\n# Override regex patterns\nconfig = DataQualityConfig.from_yaml(\"my_config.yaml\", regex_yaml_path='regex_patterns.yaml')\n\n# Running data quality checks\nreport = config.execute(data_source=my_dataframe)\n\n# Or, creating config programmatically from scratch\nconfig2 = DataQualityConfig(\n    dataset_name=\"my_data\",\n    rules=[\n        ValidityRegexRule(field=\"email\", regex_pattern='.+@example.com'),\n    ],\n)\n</code></pre> <p>Methods:</p> <pre><code>execute(data_source) -&gt; DataQualityReport:\n    Execute the measurement configuration against the provided data source\n    (e.g., pandas DataFrame, Spark DataFrame).\n    Runs each rule's evaluate() method and returns a DataQualityReport\n    containing the results.\n\nfrom_yaml(file_path: str | Path, regex_yaml_path: str | Path | None = None) -&gt; DataQualityConfig:\n    Load a configuration instance from a YAML file. If regex_yaml_path is provided,\n    regex patterns in rule definitions can be overridden or supplemented by patterns\n    from this separate YAML file.\n\nto_yaml(file_path: str | Path, overwrite: bool = False) -&gt; None:\n    Save as YAML file.\n\nfrom_report(report: DataQualityReport) -&gt; DataQualityConfig:\n    Create config instance from report results. This will extract the rule defintition from the\n    rule_data field (which is a JSON dump of all rule metadata)\n</code></pre>"},{"location":"api/#gchq_data_quality.results.models.DataQualityReport","title":"<code>gchq_data_quality.results.models.DataQualityReport</code>","text":"<p>               Bases: <code>DataQualityBaseModel</code></p> <p>A collection of individual data quality results for a dataset. This object is typically returned by executing a DataQualityConfig object, rather than instantiated directly by the user.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[DataQualityResult]</code> <p>List of individual DataQualityResults for each rule applied.</p> <p>Methods:</p> Name Description <code>to_dataframe</code> <p>Converts report results to a pandas DataFrame for analysis.</p> <code>to_json</code> <p>Serialises the report to JSON, optionally saving to file.</p> <code>from_dataframe</code> <p>Constructs a DataQualityReport from a pandas DataFrame formatted as in to_dataframe().</p> Example <pre><code>config = DataQualityConfig.from_yaml('quality_cfg.yaml')\nreport = config.execute(df) # &lt;- this is the DataQualityReport object creation step\ndf_results = report.to_dataframe(decimals=3)\nreport.to_json('results.json')\n</code></pre>"},{"location":"api/#gchq_data_quality.results.models.DataQualityResult","title":"<code>gchq_data_quality.results.models.DataQualityResult</code>","text":"<p>               Bases: <code>DataQualityBaseModel</code></p> <p>Represents the outcome of a single data quality rule applied to a dataset column. Noting that some rules may reference additional columns, such as ConsistencyRule</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>float | str | int | None</code> <p>Common, human-readable name of the measured dataset.</p> <code>dataset_id</code> <code>float | str | int | None</code> <p>Machine-readable unique ID for the dataset.</p> <code>measurement_sample</code> <code>str | None</code> <p>Description of the sample measured.</p> <code>lifecycle_stage</code> <code>Any | None</code> <p>Stage of data lifecycle at the time of measurement (e.g., '01 ingest').</p> <code>measurement_time</code> <code>UTCDateTimeStrict</code> <p>UTC timestamp when measurement was taken. Defaults to 'now' in UTC.</p> <code>field</code> <code>str</code> <p>Name of the column the rule applies to.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Data quality dimension evaluated (Uniqueness, Completeness, etc.).</p> <code>records_evaluated</code> <code>int | None</code> <p>Total records evaluated by this rule.</p> <code>pass_rate</code> <code>float | None</code> <p>Ratio (0-1) of passing records to evaluated records.</p> <code>rule_id</code> <code>Any | None</code> <p>Local identifier for the applied rule.</p> <code>rule_description</code> <code>Any</code> <p>Text, dict, or JSON describing rule parameters and logic.</p> <code>rule_data</code> <code>str</code> <p>JSON dump of rule metadata for reconstruction of rule.</p> <code>records_failed_ids</code> <code>list | None</code> <p>Up to 10 (default) identifiers for rows failing the rule.</p> <code>records_failed_sample</code> <code>list[dict] | None</code> <p>Sample output of failed records for diagnostics.</p> Example <pre><code># Typical user interaction is via DataQualityReport:\nconfig = DataQualityConfig.from_yaml('config.yaml')\nreport = config.execute(df)\nfirst_result = report.results[0]\nprint(first_result.pass_rate)  # Access result attributes\n</code></pre> Note <p>Direct construction of DataQualityResult or DataQualityReport are rare; results are typically gathered in production using RuleType.evaluate(df) or DataQualityConfig.execute(data)</p>"},{"location":"api/#spark-utilities","title":"Spark Utilities","text":""},{"location":"api/#gchq_data_quality.spark.dataframe_operations.flatten_spark","title":"<code>gchq_data_quality.spark.dataframe_operations.flatten_spark(df, flatten_cols)</code>","text":"<p>Flattens arrays and nested fields in a Spark DataFrame to produce a Spark-safe, single-level table.</p> <p>The columns to flatten may include array or struct paths, with array selections:     '[*]' - explodes arrays into multiple rows     '[]' - selects the first non-null element from the array</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame containing nested or array fields.</p> required <code>flatten_cols</code> <code>list[str]</code> <p>List of strings indicating nested columns to flatten. Paths may include array notation (e.g., 'orders[*].item', 'info.details[]').</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Spark DataFrame with the specified columns flattened and Spark-safe</p> <code>DataFrame</code> <p>column names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the column paths are inconsistent or not found in the schema, or if array notation is misapplied.</p> Example <p>Flatten three levels of orders in a customer DataFrame: <pre><code>flat_df = flatten_spark(df, [\n    \"customer[*].orders[*].items[*].productId\",\n    \"customer[*].name\"\n])\nflat_df.show()\n</code></pre></p>"},{"location":"api/#types-and-base-rule","title":"Types and Base Rule","text":"<p>The way we categorise the data quality dimensions</p> <p>The base rule is never called by a user, but serves as a parent for all data quality rules. </p>"},{"location":"api/#gchq_data_quality.models.DamaFramework","title":"<code>gchq_data_quality.models.DamaFramework</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Allowed names for data quality framework dimensions following DAMA (Data Management Association).</p> Members <p>Uniqueness: Value is \"Uniqueness\". Completeness: Value is \"Completeness\". Validity: Value is \"Validity\". Consistency: Value is \"Consistency\". Accuracy: Value is \"Accuracy\". Timeliness: Value is \"Timeliness\".</p> Note <p>It will accept any string case, but coerce to title case.</p> Example <pre><code>DamaFramework(\"uniqueness\")   # Returns DamaFramework.Uniqueness\nDamaFramework.Completeness.value  # \"Completeness\"\n</code></pre>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule","title":"<code>gchq_data_quality.rules.base.BaseRule</code>","text":"<p>               Bases: <code>DataQualityBaseModel</code>, <code>ABC</code></p> <p>Abstract base class for data quality rule definitions.</p> <p>Not intended for direct use. Use a Subclass with a specific rule type (e.g., AccuracyRule, CompletenessRule) for configuration or execution of data quality checks. BaseRule handles all generic configuration and evaluation steps, with rule-specific logic implemented via subclass overrides.</p> <p>Attributes:</p> Name Type Description <code>field</code> <code>str</code> <p>Column to check for rule evaluation.</p> <code>rule_id</code> <code>str | None</code> <p>Optional identifier for this rule.</p> <code>rule_description</code> <code>str | None</code> <p>Optional summary or explanation of the rule.</p> <code>na_values</code> <code>str | int | float | list[Any] | None</code> <p>Values to treat as NULL.</p> <code> pd.NA =  \")           class-attribute       instance-attribute    (gchq_data_quality.rules.base.BaseRule.skip_if_null)\" href=\"#gchq_data_quality.rules.base.BaseRule.skip_if_null\"&gt;skip_if_null <code>Literal['all', 'any', 'never']</code> <p>Controls what records are skipped due to nulls.</p> <code>data_quality_dimension</code> <code>DamaFramework</code> <p>Linked DAMA data quality dimension.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>pd.DataFrame | SparkDataFrame | Elasticsearch) -&gt; DataQualityResult Applies the rule to source data and returns evaluation metrics and diagnostics.</p> Note <p>This base class should not be instantiated directly. Use a rule subclass for actual configuration or evaluation.</p> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains metrics of evaluation such as pass rate,</p> <p>evaluated record count, indices/sample of failed records, and rule metadata.</p> <p>See DataQualityResult documentation for details.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule.data_quality_dimension","title":"<code>data_quality_dimension = Field(..., description='The Dama dimension for each rule')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule.field","title":"<code>field = Field(..., description='Column to check')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule.na_values","title":"<code>na_values = Field(default=None, description='Additional values to treat as null')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule.rule_description","title":"<code>rule_description = Field(default=None, description='Description of the rule')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule.rule_id","title":"<code>rule_id = Field(default=None, description='Identifier for this rule')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule.skip_if_null","title":"<code>skip_if_null = Field(default='any', description=\"Controls which rows are skipped that contain null values. If 'all' then it will only skip if all columns used are NULL.most rules this will just apply to the 'field' column, but some like TimelinessRelativeRule can use more than one column.If values aren't skipped, then NULL values are passed into the calculations so be cautious as to what you allow through as 3 &gt; pd.NA = &lt;NA&gt; \")</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule._coerce_dataframe_type","title":"<code>_coerce_dataframe_type(df)</code>","text":"<p>Some rules require values to be coerced to a different data type. Timeliness &gt; UTC datetime, ValiditiyNumericalRange &gt; numeric</p> <p>This function handles coercing to the relevant data type for the rule. Override if needed, the default behaviour is no coercion</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: If no coercion, the original df. If coerced, a modifed dataframe</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._copy_and_subset_dataframe","title":"<code>_copy_and_subset_dataframe(df, columns_used)</code>","text":"<p>Copies the dataframe to avoid later mutations when we replace NA values or coerce to a different data type.</p> <p>Also ensures the dataframe columns are kept in the same order as the orginal df</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._evaluate_in_elastic","title":"<code>_evaluate_in_elastic(es, index_name, query=None)</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule._evaluate_in_pandas","title":"<code>_evaluate_in_pandas(df)</code>","text":"<p>Evaluates the rule against the provided DataFrame.</p> <p>Performs field existence check, handles NA values and coercion calculates number of records evaluated and passing, computes pass rate, and includes a sample of failed records if required. A subset of the steps below can be overriden to give any inherited rule the desirved behaviour, without having to completely override the evaluate() function itself.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to evaluate</p> required <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <code>DataQualityResult</code> <p>A summary of data quality metrics for the rule such as</p> <code>DataQualityResult</code> <p>records evaluated, pass rate, and details of failed records if required.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._evaluate_in_pandas_output_dataframe","title":"<code>_evaluate_in_pandas_output_dataframe(df)</code>","text":"<p>Wrapper to ensure when executing in Spark we return a DataFrame (this is a Spark requirement), yet we want to maintain the behaviour that _evaluate_in_pandas returns a DataQualityResult (so did not want to override that).</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Dataframe in a format that matches SparkDataQualityResultSchema</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._evaluate_in_spark","title":"<code>_evaluate_in_spark(spark_df)</code>","text":"<p>By default we execute everything in pandas via mapInPandas, this partitions the data automatically and sends dataframes to each Spark worker, we then aggregate the resulting data.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_columns_used_pandas","title":"<code>_get_columns_used_pandas()</code>","text":"<p>The columns used in evaluting the rule, defaults to just the field, but other rules such as consistency may use more than one column and will override this.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_null_count","title":"<code>_get_null_count(df, field)</code>","text":""},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_null_counts_all_columns","title":"<code>_get_null_counts_all_columns(df)</code>","text":"<p>Goes through each column and calculates the null count.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>A dictionary of {column_name : null_count} e.g. {'name' : 7, 'age' : 0}</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_evaluated_mask_pandas","title":"<code>_get_records_evaluated_mask_pandas(df)</code>","text":"<p>The bool mask of whether a record is being evaluated. The majority of rules will not evaluate against records that are NULL With the exception of the CompletenessRule. So the default behaviour is evaluate NON null values.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_evaluated_pandas","title":"<code>_get_records_evaluated_pandas(df)</code>","text":"<p>Computes the number of records that are evaluated against the rule.</p> <p>By default, counts non-null entries in the target field. Override this for rules involving multiple columns or different completeness logic.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of records in the field being assessed.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_failed_mask_pandas","title":"<code>_get_records_failed_mask_pandas(df)</code>","text":"<p>Abstract method to generate a boolean mask for records failing the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Boolean mask where True indicates a failing record.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_failed_pandas","title":"<code>_get_records_failed_pandas(df)</code>","text":"<p>Returns a list of unique records from the field that failed the rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame instance to process (assumes df has been filtered to just contain required columns).</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict]</code> <p>Unique records from the field corresponding to failed records. In format [{colA : valueA, colB : valueB}, {...etc}]</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_passing_mask_pandas","title":"<code>_get_records_passing_mask_pandas(df)</code>  <code>abstractmethod</code>","text":"<p>The bool mask of what records are passing (i.e. this function is the main way we define our data quality rules), this is also an AND with the records_evaluated_mask by definition, as we cannot pass a record if it has not been evaluated.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_records_passing_pandas","title":"<code>_get_records_passing_pandas(df)</code>","text":"<p>Abstract method to compute the number of records passing the data quality rule.</p> <p>This must be customised for each specific rule.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of records passing the rule's criteria.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_skip_if_null_mask","title":"<code>_get_skip_if_null_mask(df)</code>","text":"<p>Return mask for records to skip based on self.skip_if_null.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._get_spark_safe_rule","title":"<code>_get_spark_safe_rule()</code>","text":"<p>Returns a modified (deep copy) of the rule with spark safe column names in any column used to evaluate the rule. This is required when working with nested data, as if we want to measure 'customers.age' after we flatten the dataframe and exract the age property from the 'customers' object our column will be renamed to customers_age when it gets passed to _evaluate_in_pandas.</p> <p>As 'customers.age' is not a valid Spark column name once the data is flattened.</p> <p>This is overridden for each subrule type if more than self.field is used</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._handle_dataframe_coercion","title":"<code>_handle_dataframe_coercion(df)</code>","text":"<p>Coerce the dataframe to a new datatype (if required). We will also check if the null count changes upon coercion and raise a warning with the user</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._handle_na_values_pandas","title":"<code>_handle_na_values_pandas(df, columns_used, na_values)</code>","text":"<p>Replace specified values in a DataFrame with pd.NA if na_values is provided.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>columns_used</code> <code>list</code> <p>Columns to scan for null-like values.</p> required <code>na_values</code> <code>list</code> <p>List of values to consider as missing.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe where the specified values are replaced with pd.NA, or the original if na_values is None.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._replace_na_in_bool_mask","title":"<code>_replace_na_in_bool_mask(mask)</code>","text":"<p>If we get 'None' in a boolean mask we can't conduct mask operations such as inverting it or logical AND / OR, this replaces None / NA, with False.</p> <p>This method can be overridden by child classes</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._require_failed_records_sample","title":"<code>_require_failed_records_sample(pass_rate)</code>","text":"<p>Determines whether a diagnostic sample of failed records should be collected.</p> <p>Parameters:</p> Name Type Description Default <code>pass_rate</code> <code>float | None</code> <p>The rule pass rate, or None if no records were evaluated.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if failed record samples are required; otherwise, False.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule._warn_if_null_counts_different","title":"<code>_warn_if_null_counts_different(original_null_counts, new_null_counts)</code>","text":"<p>Compares the null counts between the original and new (the keys will be the same), if the new has more nulls, raise a warning and mention the column. Typically something we do during coercion to a new datatype.</p>"},{"location":"api/#gchq_data_quality.rules.base.BaseRule.evaluate","title":"<code>evaluate(data_source, index_name='', query=None)</code>","text":"<pre><code>evaluate(data_source: pd.DataFrame) -&gt; DataQualityResult\n</code></pre><pre><code>evaluate(data_source: SparkDataFrame) -&gt; DataQualityResult\n</code></pre><pre><code>evaluate(\n    data_source: Elasticsearch,\n    index_name: str = ...,\n    query: dict | None = ...,\n) -&gt; DataQualityResult\n</code></pre> <p>Evaluates this rule against the provided data source.</p> <p>Supports both Pandas and Spark DataFrames as input. Applies all rule configuration, handles nulls and data coercion, and computes relevant data quality metrics. If an Elasticsearch index and client are supplied, an error is raised unless that backend is implemented. Currently not implemented.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>DataFrame | DataFrame | Elasticsearch</code> <p>The data to evaluate\u2014 can be a Pandas DataFrame, a Spark DataFrame, or an Elasticsearch client.</p> required <code>index_name</code> <code>str</code> <p>Required if evaluating with Elasticsearch; the index to check.</p> <code>''</code> <code>query</code> <code>dict</code> <p>Required if evaluating with Elaticsearch, defaults to a query that matches all documents</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataQualityResult</code> <p>Contains the metrics and diagnostics of rule evaluation,</p> <p>including pass rate, number of records evaluated, indices and sample of failed records,</p> <p>and rule metadata. See DataQualityResult documentation for details.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported data source is provided.</p> <code>NotImplementedError</code> <p>If Elasticsearch evaluation is requested but not supported.</p>"},{"location":"installation/","title":"Installation","text":"<p>The <code>gchq-data-quality</code> package is designed to be modular, allowing you to install its core feature (Pandas dataframe code) as well as optional components for Spark (<code>pyspark</code>) and/or Elasticsearch (Elasticsearch is not yet implemented).</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>gchq-data-quality</code>, ensure the following: - Python: Version 3.11 or later is required. - Package Manager: <code>pip</code> (Python's package manager) should be up-to-date. Run:   <pre><code>pip install --upgrade pip\n</code></pre></p>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#core-package","title":"Core Package","text":"<p>To install the core functionality of <code>gchq-data-quality</code> using Python's <code>pip</code>, run: <pre><code>pip install gchq-data-quality\n</code></pre></p>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#pyspark-integration","title":"PySpark Integration","text":"<p>To use <code>gchq-data-quality</code> with Apache Spark, install the package with the <code>pyspark</code> extra: <pre><code>pip install gchq-data-quality[pyspark]\n</code></pre></p> <p>This additionally installs:</p> <ul> <li>pyspark (this is quite a large package, hence having it as an optional extra)</li> </ul>"},{"location":"installation/#elasticsearch-integration","title":"Elasticsearch Integration","text":"<p>To use <code>gchq-data-quality</code> with Elasticsearch, install the package with the <code>elasticsearch</code> extra: <pre><code>pip install gchq-data-quality[elasticsearch]\n</code></pre></p> <p>This additionally installs:</p> <ul> <li>elasticsearch</li> </ul>"},{"location":"installation/#multiple-extras","title":"Multiple Extras","text":"<p>You can combine multiple optional integrations in a single command. For example, to install both PySpark and Elasticsearch support: <pre><code>pip install gchq-data-quality[pyspark,elasticsearch]\n</code></pre></p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For local development, clone the repository and install the package in editable mode:</p> <ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/gchq/gchq-data-quality.git\ncd gchq-data-quality\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol> <p>The <code>dev</code> group includes: - Testing tools (<code>pytest</code>, <code>coverage</code>) - Pre-commit hooks (<code>pre-commit</code>) - Formatting and linting tools (<code>ruff</code>) - <code>ipykernel</code> so you can edit the Tutorial Notebooks if required - Packages to build documentation with MkDocs.</p>"},{"location":"installation/#verification-and-testing","title":"Verification and Testing","text":"<p>After installation, verify that the package was installed correctly:</p> <pre><code>python -c \"import gchq_data_quality; print(gchq_data_quality.__version__)\"\n</code></pre>"},{"location":"installation/#uninstall","title":"Uninstall","text":"<p>To uninstall the package and its dependencies, run: <pre><code>pip uninstall gchq-data-quality\n</code></pre></p> <p>If you installed optional dependencies, repeat the command for each extra name (<code>pyspark</code>, <code>elasticsearch</code>, etc.).</p>"},{"location":"python-advanced/","title":"Data Quality Tutorial - Python 2","text":""},{"location":"python-advanced/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completion of the Python 1 Tutorial (comfortable running data quality functions on DataFrames).</li> </ul> <p>Coding: You should know about pandas DataFrames and basic Python syntax.</p>"},{"location":"python-advanced/#aim","title":"Aim","text":"<ul> <li>Create Data Quality config files (YAML-based rule lists).</li> <li>Run these configs directly against your DataFrames.</li> <li>Manage your regex patterns from a single YAML file</li> </ul>"},{"location":"python-advanced/#1-reusing-example-data","title":"1. Reusing Example Data","text":"<pre><code>import pandas as pd\nfrom datetime import datetime\n\ndf = pd.DataFrame({\n    \"id\": [1, 2, 3, 3, 5],\n    \"name\": [\"John\", \"Jane\", \"Dave\", None, \"Missing\"],\n    \"age\": [30, 25, 102, 15, -5],\n    \"email\": [\n        \"john@example.com\",\n        \"jane@example.com\",\n        \"dave@example\",\n        \"test@test.com\",\n        \"alice@example.com\",\n    ],\n    \"category\": [\"A\", \"B\", \"C\", \"D\", \"X\"],\n    \"score\": [10, 20, 30, 40, -1],\n    \"date\": [\n        datetime(2023, 1, 1),\n        datetime(2023, 2, 1),\n        datetime(2023, 3, 1),\n        datetime(2021, 1, 1),\n        datetime(2023, 5, 1),\n    ]\n})\n</code></pre>"},{"location":"python-advanced/#2-yaml-config-files-for-data-quality-rules","title":"2. YAML Config Files for Data Quality Rules","text":""},{"location":"python-advanced/#why-yaml","title":"Why YAML?","text":"<p>YAML is human-readable and machine-readable, well easier for a human to read than JSON anyway.</p>"},{"location":"python-advanced/#key-structure","title":"Key structure:","text":"<ul> <li>Overall metadata (e.g. dataset_name, measurement_time)     all optional</li> <li>List of rules     you must have at least one rule before you run the config.</li> </ul>"},{"location":"python-advanced/#example","title":"Example:","text":"<pre><code>dataset_name: My Source Data\nmeasurement_sample: 10% of records\nlifecycle_stage: null\nrules:\n  - field: id\n    function: uniqueness\n  - field: name\n    na_values: ''\n    function: validity_regex\n    regex_pattern: '[A-z0-9_]'\n</code></pre>"},{"location":"python-advanced/#lists-in-yaml","title":"Lists in YAML","text":"<pre><code>valid_values: [A, B, C, D]   # simple\nvalid_values:\n  - A\n  - B\n  - C\n  - D               # verbose, useful for long lists\n</code></pre>"},{"location":"python-advanced/#regex-in-yaml","title":"Regex in YAML","text":"<p>Always surround <code>regex_pattern</code> with single quotes:</p> <pre><code>regex_pattern: '[A-Za-z]+'\nregex_pattern: '\\d{4}-\\d{2}-\\d{2}'\nregex_pattern: 'don''t'    # To include a single quote\n</code></pre>"},{"location":"python-advanced/#3-loading-and-validating-config-files","title":"3. Loading and Validating Config Files","text":"<pre><code>from gchq_data_quality import DataQualityConfig\n\nconfig = DataQualityConfig.from_yaml(\"your_config.yaml\")\n</code></pre> <p>You can load multiple files. </p> <p>We find it makes sense to split your rules into separate files based on what you are measuring. For example, if you own a pet shop and have rules around pet details and customer details and orders, you might want a <code>dates.yaml</code> for storing rules relating to all your dates and <code>names.yaml</code> relating to rules for names of pets and owners:</p> <pre><code>config = DataQualityConfig.from_yaml(['dates.yaml', 'names.yaml'])\n</code></pre>"},{"location":"python-advanced/#4-running-a-config-against-your-data","title":"4. Running a Config Against Your Data","text":"<p>Once your config is loaded:</p> <pre><code>report = config.execute(df)\nprint(report.to_dataframe(measurement_time_format=\"%Y-%m-%d %H:%M\"))\n</code></pre> <p>You can adjust config metadata programmatically. It can be usfeul override <code>measurement_time</code>, as you may want to pretend the data was measured at the date of ingest, rather than when you actually measured it, as it can help make sense of your analysis later to understand the quality of the data based on what it landed.</p> <pre><code>from datetime import timezone\n\nconfig.measurement_sample = \"Test Sample\"\nconfig.dataset_name = \"Overwrite Dataset Name\"\nconfig.measurement_time = datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"python-advanced/#5-creating-a-config-file-from-a-report","title":"5. Creating a Config File From a Report","text":"<p>A typical workflow: - Experiment with rules in Python - Produce a DataQualityReport - Extract those rules back into a deployable YAML config and modify   - saves you writing out the entire YAML file from scratch</p> <pre><code>config_from_report = DataQualityConfig.from_report(report)\nconfig_from_report.to_yaml(\"yaml_from_report.yaml\", overwrite=True)\n</code></pre>"},{"location":"python-advanced/#6-mangaing-regular-expressions","title":"6. Mangaing Regular Expressions","text":"<p>Use a separate YAML file for regex patterns, to keep config rules readable and maintainable.</p> <p><code>regex_patterns.yaml</code>: <pre><code>EMAIL_REGEX: '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\nPHONE_REGEX: '[0-9]+'\n</code></pre></p> <p>Reference pattern names instead of raw regex in your rules: <pre><code>- field: email\n  function: validity_regex\n  regex_pattern: EMAIL_REGEX\n</code></pre></p> <p>When loading your config:</p> <pre><code>config = DataQualityConfig.from_yaml(\n    \"config_with_regex_refs.yaml\", \n    regex_yaml_path=\"regex_patterns.yaml\"\n)\n</code></pre>"},{"location":"python-advanced/#7-tweak-output-display-advanced","title":"7. Tweak Output Display (Advanced)","text":"<p>Control sample output size globally:</p> <pre><code>from gchq_data_quality.globals import SampleConfig\nSampleConfig.RECORDS_FAILED_SAMPLE_SIZE = 25\n</code></pre>"},{"location":"python-basic/","title":"Data Quality Tutorial - Python 1","text":""},{"location":"python-basic/#prerequisites","title":"Prerequisites","text":"<ul> <li>Familiarity with pandas DataFrames.</li> <li>Have read What is Data Quality</li> </ul> <p>You should therefore already know:</p> <ul> <li>What the 6 DAMA data quality dimensions are.</li> <li>What the pass rate of a completeness rule means.</li> </ul>"},{"location":"python-basic/#aim","title":"Aim","text":"<ul> <li>Run all 8 data quality rules on a DataFrame.</li> <li>Export results to CSV.</li> </ul>"},{"location":"python-basic/#1-create-sample-dataframe","title":"1. Create Sample DataFrame","text":"<p>We'll use a purposely \"messy\" dataset to demonstrate data quality rules. Perhaps this is a list of members for your local sports club.</p> <pre><code>import pandas as pd\nfrom datetime import datetime, timedelta\n\ndf = pd.DataFrame({\n    \"id\": [1, 2, 3, 3, 5],  # Duplicate ID\n    \"name\": [\"John\", \"Jane\", \"Dave\", None, \"Missing\"],  # Missing value\n    \"age\": [30, 25, 102, 15, -5],  # Implausible negative age\n    \"email\": [\n        \"john@example.com\",\n        \"jane@example.com\",\n        \"dave@example\",\n        \"test@test.com\",\n        \"alice@example.com\",\n    ],  # Malformed email\n    \"category\": [\"A\", \"B\", \"C\", \"D\", \"X\"],  # 'X' not allowed\n    \"score\": [10, 20, 30, 40, -1],  # -1 as a missing value marker\n    \"date\": [\n        datetime(2023, 1, 1),\n        datetime(2023, 2, 1),\n        datetime(2023, 3, 1),\n        datetime(2021, 1, 1),  # Outside of recent range\n        datetime(2023, 5, 1),\n    ]\n})\ndf.head()\n</code></pre>"},{"location":"python-basic/#2-apply-data-quality-rules","title":"2. Apply Data Quality Rules","text":"<p>We\u2019ll use the package\u2019s data quality rules and report classes. Import them:</p> <pre><code>from gchq_data_quality import (\n    AccuracyRule,\n    CompletenessRule,\n    ConsistencyRule,\n    TimelinessRelativeRule,\n    TimelinessStaticRule,\n    UniquenessRule,\n    ValidityNumericalRangeRule,\n    ValidityRegexRule,\n\n    DataQualityConfig,\n    DataQualityReport,\n)\n\n# or\n\nfrom gchq_data_quality import * # this will just import those functions above\n</code></pre> <p>Initialise a report to collect the results:</p> <pre><code>FINAL_REPORT = DataQualityReport()\n</code></pre>"},{"location":"python-basic/#21-uniquenessrule","title":"2.1 UniquenessRule","text":"<p>Checks for duplicate values in a column.</p> <pre><code>uniqueness_rule = UniquenessRule(field=\"id\")\ndq_result = uniqueness_rule.evaluate(df)\nprint(f\"Uniqueness pass rate: {dq_result.pass_rate}\")\nprint(f\"Rows with duplicate ids: {dq_result.records_failed_ids}\")\n</code></pre> <p>Add to report: <pre><code>FINAL_REPORT.results.append(dq_result)\n</code></pre></p>"},{"location":"python-basic/#22-completenessrule","title":"2.2 CompletenessRule","text":"<p>Checks for missing/incomplete values.</p> <pre><code>completeness_rule = CompletenessRule(field=\"name\")\ncompleteness_result = completeness_rule.evaluate(df)\nprint(f\"Completeness pass rate: {completeness_result.pass_rate}\")\n</code></pre> <p>To treat <code>\"Missing\"</code> as a null/missing value: <pre><code>completeness_rule.na_values = [\"Missing\"]\ncompleteness_result = completeness_rule.evaluate(df)\nprint(f\"With 'Missing' as null: {completeness_result.pass_rate}\")\n</code></pre> Add to report: <pre><code>FINAL_REPORT.results.append(completeness_result)\n</code></pre></p>"},{"location":"python-basic/#23-accuracyrule","title":"2.3 AccuracyRule","text":"<p>Checks if values come from a list of valid entries. Useful for known values like ISO Country Codes. Or perhaps in our Sports Club example, the sports people play e.g. <code>['Football', 'Hockey']</code> - this ensures consistent spelling and captilisation is checked in your data</p> <pre><code>accuracy_rule = AccuracyRule(field=\"category\", valid_values=[\"A\", \"B\", \"C\", \"D\"])\naccuracy_result = accuracy_rule.evaluate(df)\nprint(f\"Accuracy pass rate: {accuracy_result.pass_rate}\")\nprint(f\"Values not in allowed list: {accuracy_result.records_failed_sample}\")\n</code></pre> <p>Inverse check: to flag presence of forbidden values. <pre><code>accuracy_rule.inverse = True\nforbidden_result = accuracy_rule.evaluate(df)\nprint(f\"Presence of forbidden values: {forbidden_result.records_failed_sample}\")\n</code></pre> Add initial result to the report: <pre><code>FINAL_REPORT.results.append(accuracy_result)\n</code></pre></p>"},{"location":"python-basic/#24-consistencyrule","title":"2.4 ConsistencyRule","text":"<p>Checks logical relationships (often between columns).</p> <p>Expression Syntax: Use backticks around all column names in your expressions.</p> <p>You can experiment with expressions directly in a pandas dataframe using: <code>df.eval(\"&lt;your expression here&gt;\")</code></p> <p><pre><code># Simple numeric rule\nconsistency_rule = ConsistencyRule(field=\"age\", expression=\"`age` &gt; 3\")\nconsistency_result = consistency_rule.evaluate(df)\nprint(f\"Consistency pass rate: {consistency_result.pass_rate}\")\n\n# Compound rule: if-then\n# This rule will then ONLY be evaluated against rows that match the 'if' statement, skipping others\nconsistency_rule2 = ConsistencyRule(\n    field=\"age\", expression={\"if\": \"`age` &gt; 3\", \"then\": \"`score` &lt;= 40\"}\n)\nconsistency_result2 = consistency_rule2.evaluate(df)\nprint(f\"Constrained pass rate: {consistency_result2.pass_rate}\")\n</code></pre> Add to report: <pre><code>FINAL_REPORT.results.append(consistency_result)\nFINAL_REPORT.results.append(consistency_result2)\n</code></pre></p>"},{"location":"python-basic/#handling-nulls","title":"Handling Nulls","text":"<p><code>skip_if_null</code> can be <code>\"any\"</code> (default), <code>\"all\"</code>, or <code>\"never\"</code>.</p> <pre><code>consistency_rule.skip_if_null = \"never\"\nnever_skip_result = consistency_rule.evaluate(df)\nprint(f\"Pass rate with nulls included: {never_skip_result.pass_rate}\")\n</code></pre>"},{"location":"python-basic/#25-timeliness-rules","title":"2.5 Timeliness Rules","text":"<p>Check if dates fall inside certain ranges (inclusively). If no time-zone is provided it will assume UTC.</p> <p>You can pass in strings or datetime objects.</p>"},{"location":"python-basic/#25a-timelinessstaticrule","title":"2.5A TimelinessStaticRule","text":"<pre><code>timeliness_static_rule = TimelinessStaticRule(\n    field=\"date\", start_date=\"2023-01-01\", end_date=datetime(2023, 6, 2)\n)\ntimeliness_static_result = timeliness_static_rule.evaluate(df)\nprint(f\"Timeliness (static) pass rate: {timeliness_static_result.pass_rate}\")\nFINAL_REPORT.results.append(timeliness_static_result)\n</code></pre> <p>Note: if you provide no times, then it assumes 00:00hrs, so take care when thinking about your date boundaries, as pretty much all datetime values on 1st Jan 2023 will be later than 00:00hrs!</p> <p>The above example will check that the 'date' field occurs sometime on 2023-01-01 00:00:00 -&gt; 2023-01-01 23:59:59</p>"},{"location":"python-basic/#25b-timelinessrelativerule","title":"2.5B TimelinessRelativeRule","text":"<p>The TimelinessRelativeRule checks whether a date column falls within a time window defined relative to another date. This is useful when you need to validate dates that depend on a reference, such as ensuring bookings occur within N days of the order date.</p>"},{"location":"python-basic/#options","title":"Options","text":"<ul> <li>reference_date:<ul> <li>String date (<code>\"2023-01-01\"</code>), a <code>datetime</code> object, or <code>'now'</code> (UTC now).</li> </ul> </li> <li>reference_column:<ul> <li>Only use if <code>reference_date</code> is left blank.</li> <li>Compares each row\u2019s field against a date in another column (e.g. <code>delivery_date</code> versus <code>order_date</code>). i.e. the <code>reference_date</code> for each  record comes from another column in the same row in the dataset.</li> </ul> </li> <li>start_timedelta and end_timedelta:<ul> <li>Allowed time window before/after reference.</li> <li>Such that your start boundary is <code>reference_date + start_timedelta</code>, and end boundary is <code>reference_date + end_timedelta</code>.</li> <li>So a negative <code>start_timedelta</code> is needed to set a boundary before the <code>reference date</code>.</li> <li>Accepts <code>timedelta(days=5)</code>, <code>'5d'</code>, <code>'P5D'</code>, <code>'+6h'</code>, etc.</li> <li><code>'0d'</code> or <code>timedelta(0)</code> means your boundary is equal to the <code>reference_date</code> (or corresponding value in the <code>reference_column</code>)</li> </ul> </li> </ul>"},{"location":"python-basic/#examples","title":"Examples","text":"<p>1. Date must be within 2 years after reference date: <pre><code>from datetime import timedelta\ntimeliness_relative_rule = TimelinessRelativeRule(\n    field=\"date\",\n    reference_date=\"2023-01-01\",\n    start_timedelta=0,\n    end_timedelta=timedelta(days=365 * 2),\n)\ntimeliness_relative_result = timeliness_relative_rule.evaluate(df)\nprint(f\"Timeliness (relative) pass rate: {timeliness_relative_result.pass_rate}\")\nFINAL_REPORT.results.append(timeliness_relative_result)\n</code></pre></p> <p>2. Date must be within 5 days before to 6 hours after reference: <pre><code>timeliness_relative_rule = TimelinessRelativeRule(\n    field=\"date\",\n    reference_date=\"2023-01-01\",\n    start_timedelta='-5d',      # 5 days before\n    end_timedelta='+6h',        # up to 6 hours after\n)\n</code></pre></p> <p>3. Compare against another column (per-row): Suppose your DataFrame has <code>delivery_date</code> and <code>order_date</code>. Check that <code>delivery_date</code> is within 10 days after <code>order_date</code>:</p> <p><pre><code>timeliness_relative_rule = TimelinessRelativeRule(\n    field=\"delivery_date\",\n    reference_column=\"order_date\",\n    start_timedelta='0d',\n    end_timedelta='10d',\n)\n</code></pre> Each row's <code>delivery_date</code> is checked for being on/after <code>order_date</code> and no more than 10 days later.</p>"},{"location":"python-basic/#notes","title":"Notes","text":"<ul> <li>If only <code>start_timedelta</code> is set, the upper boundary is infinite</li> <li>If only <code>end_timedelta</code> is set, the lower boundary is infinite</li> <li>Both can be combined for an allowed range.</li> <li>ISO8601 durations (e.g., <code>'P5D'</code> for 5 days, <code>'PT6H'</code> for 6 hours) are also accepted.</li> </ul>"},{"location":"python-basic/#26-validityregexrule","title":"2.6 ValidityRegexRule","text":"<p>Checks if strings follow a pattern (e.g., valid email). See our Advanced Tutorial for how to manage your Regex patterns for production.</p> <pre><code>validity_regex_rule = ValidityRegexRule(\n    field=\"email\", regex_pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n)\nvalidity_regex_result = validity_regex_rule.evaluate(df)\nprint(f\"Regex validity pass rate: {validity_regex_result.pass_rate}\")\nFINAL_REPORT.results.append(validity_regex_result)\n</code></pre>"},{"location":"python-basic/#27-validitynumericalrangerule","title":"2.7 ValidityNumericalRangeRule","text":"<p>Checks if numbers fall within a range. If left blank / None it replaces with -infinity (min_value) or +infinity (max_value)</p> <pre><code>validity_numerical_range_rule = ValidityNumericalRangeRule(\n    field=\"age\", min_value=1, max_value=120\n)\nvalidity_numerical_range_result = validity_numerical_range_rule.evaluate(df)\nprint(f\"Numerical range validity pass rate: {validity_numerical_range_result.pass_rate}\")\nFINAL_REPORT.results.append(validity_numerical_range_result)\n</code></pre>"},{"location":"python-basic/#3-combine-export-results","title":"3. Combine &amp; Export Results","text":"<p>You can turn your results into a DataFrame (or other formats) for reporting:</p> <pre><code>df_report = FINAL_REPORT.to_dataframe()\ndf_report.to_csv(\"data_quality_report.csv\", index=False)\n</code></pre> <p>Format options for display/reporting, e.g. to make Excel row numbers line up with the records_failed_ids we can shift them. <pre><code>df_report = FINAL_REPORT.to_dataframe(\n    decimals=2, \n    measurement_time_format=\"%Y-%m-%d %H:%M\", \n    records_failed_ids_shift=2\n)\n</code></pre></p>"},{"location":"python-basic/#4-dataqualityconfig-optionaladvanced","title":"4. DataQualityConfig (optional/advanced)","text":"<p>Instead of manually appending, you can define rule sets as a config object:</p> <pre><code>dq_config = DataQualityConfig(\n    dataset_name=\"Tutorial Data\",\n    lifecycle_stage=\"02 Post Processing\",\n    measurement_time=\"2025-01-01\",\n    rules=[\n        uniqueness_rule,\n        completeness_rule,\n        accuracy_rule,\n        timeliness_relative_rule,\n        timeliness_static_rule,\n        consistency_rule,\n        validity_numerical_range_rule,\n        validity_regex_rule,\n    ],\n)\n\ndq_report = dq_config.execute(df)\ndf_report = dq_report.to_dataframe()\ndf_report.to_csv(\"data_quality_report.csv\", index=False)\n</code></pre>"},{"location":"python-basic/#5-next-steps","title":"5. Next Steps","text":"<ul> <li> <p>To learn about advanced configuration, using YAML rules files, and automation, continue with Advanced Python Tutorial.</p> </li> <li> <p>Generate rule definitions from your report using:     <pre><code>dc = DataQualityConfig.from_report(dq_report)\ndc.to_yaml(\"rules.yaml\", overwrite=True) # save the rules to a YAML file and use this a fast way of creating a template for changing them\n</code></pre></p> </li> </ul>"},{"location":"python-pyspark/","title":"Data Quality Tutorial - PySpark","text":""},{"location":"python-pyspark/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have worked through Python 1 and Python 2 Tutorials:<ul> <li>Able to run and interpret data quality functions.</li> <li>Comfortable managing YAML rule configs.</li> </ul> </li> <li>Basic experience with Spark and pandas DataFrames.</li> </ul>"},{"location":"python-pyspark/#aim","title":"Aim","text":"<ul> <li>Run data quality rules directly on Spark DataFrames.</li> <li>Understand how nested data is handled in Spark within this package.</li> </ul>"},{"location":"python-pyspark/#1-example-data","title":"1. Example Data","text":"<p>Create the test dataset (same as Python 1, but for Spark):</p> <pre><code>import pandas as pd\nfrom datetime import datetime\nfrom pyspark.sql import SparkSession\n\ndf = pd.DataFrame({\n    \"id\": [1, 2, 3, 3, 5],\n    \"name\": [\"John\", \"Jane\", \"Dave\", None, \"Missing\"],\n    \"age\": [30, 25, 102, 15, -5],\n    \"email\": [\n        \"john@example.com\",\n        \"jane@example.com\",\n        \"dave@example\",\n        \"test@test.com\",\n        \"alice@example.com\",\n    ],\n    \"category\": [\"A\", \"B\", \"C\", \"D\", \"X\"],\n    \"score\": [10, 20, 30, 40, -1],\n    \"date\": [\n        datetime(2023, 1, 1),\n        datetime(2023, 2, 1),\n        datetime(2023, 3, 1),\n        datetime(2021, 1, 1),\n        datetime(2023, 5, 1),\n    ]\n})\n\nspark = SparkSession.builder.appName(\"My App\").getOrCreate()\ndfs = spark.createDataFrame(df)\ndfs.show()\n</code></pre>"},{"location":"python-pyspark/#2-running-data-quality-rules-in-spark","title":"2. Running Data Quality Rules in Spark","text":"<p>The API is identical to pandas</p> <pre><code>from gchq_data_quality import UniquenessRule, TimelinessStaticRule\n\nuniqueness_rule = UniquenessRule(field=\"id\")\ndq_result = uniqueness_rule.evaluate(dfs)\nprint(dq_result.model_dump())\n</code></pre>"},{"location":"python-pyspark/#note","title":"Note:","text":"<ul> <li>Under the hood, we split the data into lots of small dataframes and pass to a Spark worker (with the exception of UniquenessRule which we measure using native Spark). This was done to minimise our codebase - we can reuse the majority of our pandas code.</li> <li><code>records_failed_ids</code> is not returned for Spark DataFrames (Spark DataFrames are inherently unordered).</li> <li>You will not get python objects in <code>records_failed_samples</code> but rather a JSON serialisation of them, e.g. datetime strings rather than datetime objects</li> </ul>"},{"location":"python-pyspark/#running-a-whole-yaml-config-in-spark","title":"Running a Whole YAML Config in Spark","text":"<p>You can re-use your YAML config and regex pattern files:</p> <pre><code>from gchq_data_quality import DataQualityConfig\n\ndq_config = DataQualityConfig.from_yaml(\n    file_paths=\"SOLUTION_rules_with_regex.yaml\",\n    regex_yaml_path=\"regex_patterns.yaml\"\n)\ndq_report = dq_config.execute(dfs)\ndq_report.to_dataframe()\n</code></pre> <p>You can also repartition the Spark DataFrame to control the parallelism:</p> <pre><code>dfs_2 = dfs.repartition(2) # override Spark's default\ndq_report = dq_config.execute(dfs_2)\ndq_report.to_dataframe()\n</code></pre>"},{"location":"python-pyspark/#3-under-the-hood","title":"3. Under the Hood","text":"<ul> <li>Pandas &amp; Spark parity: Most rules are run by partitioning Spark data to small pandas DataFrames, processed in parallel using <code>mapInPandas</code>. Therefore take care with any expression in your ConsistencyRule that might use values like <code>ColumnA.mean()</code> - as this mean value will be based on a parition, not the whole dataset!</li> <li>Uniqueness: Runs in pure Spark (not split by partition, as we need knowledge of all values).</li> <li>Measurement time: Each partition is measured independently; <code>measurement_time</code> from the latest partition is used when we aggregate the results back.</li> </ul>"},{"location":"python-pyspark/#4-handling-nested-data","title":"4. Handling Nested Data","text":"<p>A unique strength of this package is support for deeply nested data (arrays, structs).</p>"},{"location":"python-pyspark/#example-pet-shop-customers","title":"Example: Pet Shop Customers","text":"<pre><code>from pyspark.sql.types import *\n\ndata = [\n    {\n        \"id\": 1,\n        \"customers\": {\n            \"name\": \"John\",\n            \"age\": 30,\n            \"pets\": [\n                {\n                    \"name\": \"Fido\",\n                    \"appointments\": [\n                        {\"date\": \"2022-01-01\", \"comment\": \"Fido First appointment\"},\n                        {\"date\": \"2022-01-02\", \"comment\": \"Fido Second appointment\"},\n                    ],\n                },\n                {\n                    \"name\": \"Whiskers\",\n                    \"appointments\": [\n                        {\"date\": \"2022-02-03\", \"comment\": \"Whiskers First appointment\"},\n                        {\n                            \"date\": \"2022-02-04\",\n                            \"comment\": \"Whiskers Second appointment\",\n                        },\n                    ],\n                },\n            ],\n        },\n    },\n    {\n        \"id\": 2,\n        \"customers\": {\n            \"name\": \"Jane\",\n            \"age\": 25,\n            \"pets\": [{\"name\": \"Rex\", \"appointments\": []}],\n        },\n    },\n    {\n        \"id\": 3,\n        \"customers\": {\n            \"name\": \"Mr No Pets\",\n            \"age\": 102,\n            \"pets\": [{\"name\": None, \"appointments\": []}],\n        },\n    },\n    {\n        \"id\": 4,\n        \"customers\": {\n            \"name\": \"Mrs Missing Pets\",\n            \"age\": 15,\n            \"pets\": [\n                {\"name\": \"missing\", \"appointments\": [{\"date\": None, \"comment\": \"none\"}]}\n            ],\n        },\n    },\n]\n\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), True),\n        StructField(\n            \"customers\",\n            StructType(\n                [\n                    StructField(\"name\", StringType(), True),\n                    StructField(\"age\", IntegerType(), True),  # &lt;-- added age to schema\n                    StructField(\n                        \"pets\",\n                        ArrayType(\n                            StructType(\n                                [\n                                    StructField(\"name\", StringType(), True),\n                                    StructField(\n                                        \"appointments\",\n                                        ArrayType(\n                                            StructType(\n                                                [\n                                                    StructField(\n                                                        \"date\", StringType(), True\n                                                    ),\n                                                    StructField(\n                                                        \"comment\", StringType(), True\n                                                    ),\n                                                ]\n                                            )\n                                        ),\n                                        True,\n                                    ),\n                                ]\n                            )\n                        ),\n                        True,\n                    ),\n                ]\n            ),\n            True,\n        ),\n    ]\n)\ndf_pets = spark.createDataFrame(data, schema=schema)\ndf_pets.printSchema()\ndf_pets.show()\n</code></pre>"},{"location":"python-pyspark/#flattening-and-referencing-nested-data","title":"Flattening and Referencing Nested Data","text":"<ul> <li> <p>Data is automatically flattened for DQ checks. You can inspect this flattening process beforehand using <code>flatten_spark</code>.</p> </li> <li> <p>Use \"dotted\" field notation, with:</p> <ul> <li><code>[*]</code> for \"all values\"</li> <li><code>[]</code> for \"first non-null value\"</li> </ul> </li> </ul> <p>Examples:</p> <pre><code>field: customers.pets[*].name           # every pet's name for customers\nfield: customers.pets[].name            # just the first pet's name (if any)\n</code></pre> <p>Column name translation when flattened:</p> <ul> <li><code>.</code> becomes <code>_</code></li> <li><code>[*]</code> becomes <code>_all</code></li> <li><code>[]</code> becomes <code>_first</code></li> </ul> <p>So <code>customers.pets[*].name</code> becomes <code>customers_pets_all_name</code> in the flattened DataFrame.</p> <p>You can flatten explicitly to check:</p> <pre><code>from gchq_data_quality.spark.dataframe_operations import flatten_spark\n\ndf_flat = flatten_spark(df_pets, flatten_cols=[\"id\", \"customers.name\", \"customers.pets[*].name\"])\ndf_flat.show()\n</code></pre>"},{"location":"python-pyspark/#writing-dq-rules-for-nested-data","title":"Writing DQ Rules for Nested Data","text":"<p>In your YAML configuration, use nested notation for <code>field</code>:</p> <pre><code>- field: customers.pets[].name\n  function: completeness\n- field: customers.age\n  function: validity_numerical_range\n  min_value: 18\n  max_value: 120\n- field: customers.pets[*].appointments[*].date\n  function: timeliness_static\n  start_date: 2022-01-01\n  end_date: 2023-01-01\n- field: customers.name\n  function: consistency\n  expression:\n    if: '`customers.age` &lt; 18'\n    then: '~`customers.name`.str.startswith(\"Mr\")'\n</code></pre> <p>Remember your backticks around all column names</p> <p>Run the config as before:</p> <pre><code>nested_config = DataQualityConfig.from_yaml(\n    \"nested_data_rules.yaml\",\n    regex_yaml_path=\"regex_patterns.yaml\"\n)\ndq_pets_nested_report = nested_config.execute(df_pets)\ndq_pets_nested_report.to_dataframe()\n</code></pre>"},{"location":"python-pyspark/#5-pyspark-specifics-tips","title":"5. PySpark Specifics &amp; Tips","text":"<ul> <li>It's OK to sample, you don't need to measure all your data to get a repeatable data quality pass rate; experiment to find the best sample size.</li> <li>No records failed IDS (row numbers): Invalid row numbers are not reported (Spark DataFrames are unordered).</li> <li>Column name mapping: After flattening, columns are renamed (<code>.</code> -&gt; <code>_</code>, etc). If generating configs from a report, you may have to \"reverse\" the renaming when moving from report output to config file.</li> <li>DataFrame-wide statistics: Consistency rules using group statistics can be unreliable on partitioned data (e.g. <code>col1 &lt;= other_col.mean()</code>). For reliable results, repartition to a single worker or precompute the statistic.</li> <li>Timezones: Always coerce to UTC. Spark can assign local timezones in datetimes without a timezone (depending on local configuration) and cause subtle errors in DQ Timeliness rules.</li> </ul>"},{"location":"python-pyspark/#6-production-ready","title":"6. Production Ready!","text":"<p>Thank you for getting this far</p>"},{"location":"what-is-data-quality/","title":"What is Data Quality?","text":"<p>Data quality refers to how well data meets the expectations and needs of its consumers. Data is considered high quality when it is fit-for-purpose\u2014in other words, when it supports its intended use effectively and reliably. It is unlikely that the quality of data will ever be perfect.</p> <p>In practice, we define data quality as the percentage of records that pass a set of data quality rules you define, based on how you will use the data.</p>"},{"location":"what-is-data-quality/#the-dama-six-dimensions-of-data-quality","title":"The DAMA Six Dimensions of Data Quality","text":"<p>We follow the DAMA Framework, grouping rules under six core dimensions. We will consider a company measuring its HR records in our examples:</p> Dimension Practical Definition Uniqueness Checks there are no duplicates (e.g., employee number should be unique). Completeness Ensures each required value is present (i.e., not NULL, blank, or 'N/A'). Accuracy Data correctly reflects the real-world \"truth\" (usually by checking against an authoritative source or by spot-checking). Example: using ISO country codes, verifying birth dates from certificates. In our code, we just check against a known list Validity Data matches expected syntax (format, type, or range)\u2014e.g., positive ages, valid email addresses \u2014 regular expressions or range checks. Timeliness Data is up-to-date and times are plausible (e.g., no future birth dates, all records \u201cfresh\u201d enough to be useful). Consistency Logical relationships between data points hold true (e.g., date of birth is before date of death; codes match reference values in other datasets)."},{"location":"what-is-data-quality/#how-we-measure-data-quality","title":"How We Measure Data Quality","text":"<p>For each rule:</p> <ul> <li>The pass rate is a number from 0 to 1 (the proportion of checked records that pass).</li> <li>Pass rates can be aggregated to track trends over time, or compare across fields and dimensions.</li> <li>Monitoring these pass rates, between dimensions and rules, helps diagnose and improve data pipelines. For example, if the uniqueness score drops with no other changes, it's likely you are getting duplicate records. If completeness and validity both plummet, it's likely your ingest pipeline needs fixing.</li> </ul>"},{"location":"what-is-data-quality/#data-quality-report-columns","title":"Data Quality Report Columns","text":"<p>A Data Quality Report contains:</p> Field Description <code>dataset_name</code> Human-readable name for your dataset. <code>dataset_id</code> Machine-readable identifier for the dataset. <code>measurement_sample</code> Description of the sample (e.g. \u201c10% records\u201d). <code>lifecycle_stage</code> Stage in the data lifecycle being measured (e.g., \u201c01_ingest\u201d, \u201c02_enrich\u201d). <code>measurement_time</code> Timestamp (UTC) of when the measurement was taken. <code>field</code> The specific field/column being checked. <code>data_quality_dimension</code> Which DAMA dimension this rule measures (e.g., Completeness, Validity). <code>rule_id</code> Short machine-readable identifier for the rule. <code>rule_description</code> Human-readable explanation (e.g., \u201cvalue matches email pattern\u201d). <code>rule_data</code> Full machine-readable rule definition (enough to recreate the rule). We store as JSON. <code>pass_rate</code> Fraction of records that passed this rule (between 0 and 1). <code>records_evaluated</code> Number of records checked for this rule. <code>records_failed_ids</code> List of indices (row numbers) of failed records (for troubleshooting). <code>records_failed_sample</code> Example values from records that failed (diagnostic sample)."},{"location":"what-is-data-quality/#why-data-quality-matters","title":"Why Data Quality Matters","text":"<p>This will be obvious to anyone who tries to analyse poor quality data! A few examples are:</p> <ul> <li>Increased efficiency \u2013 High-quality data reduces time spent cleaning and fixing downstream.</li> <li>Fewer errors and inconsistencies \u2013 Helps catch upstream schema changes, and other problems early.</li> <li>Improved compliance \u2013 Satisfies internal, legal, or external data quality requirements. e.g. retention limits of records.</li> </ul> <p>Whilst measuring the quality of data takes some work, overall it will save you time. Have an outage and want to know if the data has changed when it's back online? If you run the same rules and get the same answer, you can have an evidence-based answer in seconds.</p>"}]}