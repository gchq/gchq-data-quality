{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tutorial - Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisities\n",
    "You should have worked through:\n",
    "1. The Data Quality Core training module which is a foundation for all data quality training. This will explain the Data Quality Dimensions (like Uniqueness), and how each measure is calculated.\n",
    "2. The Python 1 Tutorial (found in this directory). This will equip you to run all of the data quality functions against dataframes, interpret the output, and export the final report\n",
    "3. The Python 2 Tutorial (found in this directory). This will show you how to store and run data quality rules from a config file.\n",
    "\n",
    "### Prior coding experience required\n",
    "* You should know what pandas dataframes are\n",
    "* You should have some basic python knowledge\n",
    "* You should be familiar with Spark\n",
    "\n",
    "## Aims of Pyspark Tutorial\n",
    "* To be able to run data quality rules in Spark (and understand how this is done)\n",
    "* To be able to run data quality rules on nested data in Spark\n",
    "    * No open source data quality package supports nested data, and this is one of the primary reasons for writing our own package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:06.275487Z",
     "iopub.status.busy": "2025-09-18T12:52:06.275193Z",
     "iopub.status.idle": "2025-09-18T12:52:11.549010Z",
     "shell.execute_reply": "2025-09-18T12:52:11.547875Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "from gchq_data_quality import (\n",
    "    ConsistencyRule,\n",
    "    DataQualityConfig,\n",
    "    TimelinessStaticRule,\n",
    "    UniquenessRule,\n",
    ")\n",
    "\n",
    "## - SPARK IMPORTS - ##\n",
    "from gchq_data_quality.spark.dataframe_operations import flatten_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-use the same dataframe we used in Python 1, but we will make it a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:11.552450Z",
     "iopub.status.busy": "2025-09-18T12:52:11.552076Z",
     "iopub.status.idle": "2025-09-18T12:52:11.579823Z",
     "shell.execute_reply": "2025-09-18T12:52:11.579301Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2, 3, 3, 5],  # 4 /5 unique\n",
    "        \"name\": [\"John\", \"Jane\", \"Dave\", None, \"Missing\"],  # 1 null value\n",
    "        \"age\": [30, 25, 102, 15, -5],  # a negative age\n",
    "        \"email\": [\n",
    "            \"john@example.com\",\n",
    "            \"jane@example.com\",\n",
    "            \"dave@example\",\n",
    "            \"test@test.com\",\n",
    "            \"alice@example.com\",\n",
    "        ],  # invalid 3rd email\n",
    "        \"category\": [\"A\", \"B\", \"C\", \"D\", \"X\"],\n",
    "        \"score\": [\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            40,\n",
    "            -1,\n",
    "        ],  # missing scores are defined as -1\n",
    "        \"date\": [\n",
    "            datetime(2023, 1, 1),\n",
    "            datetime(2023, 2, 1),\n",
    "            datetime(2023, 3, 1),\n",
    "            datetime(2021, 1, 1),  # one date too old\n",
    "            datetime(2023, 5, 1),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:11.627252Z",
     "iopub.status.busy": "2025-09-18T12:52:11.627029Z",
     "iopub.status.idle": "2025-09-18T12:52:19.844967Z",
     "shell.execute_reply": "2025-09-18T12:52:19.844038Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My App\").getOrCreate()\n",
    "\n",
    "dfs = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:19.848133Z",
     "iopub.status.busy": "2025-09-18T12:52:19.847884Z",
     "iopub.status.idle": "2025-09-18T12:52:23.134838Z",
     "shell.execute_reply": "2025-09-18T12:52:23.133858Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:23.137666Z",
     "iopub.status.busy": "2025-09-18T12:52:23.137438Z",
     "iopub.status.idle": "2025-09-18T12:52:23.142717Z",
     "shell.execute_reply": "2025-09-18T12:52:23.141977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check it has parsed a sensible schema\n",
    "print(dfs.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a basic Data Quality rule in Spark\n",
    "The approach is identical to using a dataframe. We\n",
    "1. Create our rule\n",
    "2. Evaluate against the spark dataframe\n",
    "\n",
    "Our code will determine what type of dataframe you have submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_rule = UniquenessRule(field=\"id\")\n",
    "dq_result = uniqueness_rule.evaluate(dfs)\n",
    "dq_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting failed samples - differences in Spark\n",
    "\n",
    "Spark Dataframes are unordered, so recording the records_failed_ids would be misleading, so these are not provided (you will notice above that they are None)\n",
    "\n",
    "In Spark, due to limitations on having strictly defined schemas, we ouput string versions of failed records (you will notice the failed date samples in the cell below are note a datetime object - as they are in pandas - but the string equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeliness_static_rule = TimelinessStaticRule(\n",
    "    field=\"date\", start_date=\"2023-01-01\", end_date=datetime(2023, 6, 1)\n",
    ")\n",
    "timeliness_static_result = timeliness_static_rule.evaluate(dfs)\n",
    "timeliness_static_result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute entire DataQualityConfig objects against the spark dataframe in the same way as pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:23.145954Z",
     "iopub.status.busy": "2025-09-18T12:52:23.145719Z",
     "iopub.status.idle": "2025-09-18T12:52:23.159732Z",
     "shell.execute_reply": "2025-09-18T12:52:23.159125Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_config = DataQualityConfig.from_yaml(\n",
    "    file_paths=\"resources/SOLUTION_rules_with_regex.yaml\",\n",
    "    regex_yaml_path=\"resources/regex_patterns.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:23.162402Z",
     "iopub.status.busy": "2025-09-18T12:52:23.162157Z",
     "iopub.status.idle": "2025-09-18T12:52:34.362276Z",
     "shell.execute_reply": "2025-09-18T12:52:34.361525Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_report = dq_config.execute(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_report.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Note:\n",
    "We do not return invalid_row_numbers (these values will remain empty), as spark dataframes are inherently unordered, and the values will not make much sense due to how we partition the data (see below)\n",
    "\n",
    "## What is happening within Spark\n",
    "The way we execute data quality rules in spark is as follows:\n",
    "1. Spark will partition the data into lots of small pandas dataframes and run the same code that we use on pandas against each dataframe individually.\n",
    "    * We use the 'mapInPandas' method\n",
    "    * The size of the partions is decided by Spark, but you can overwride with the num_groups argument.\n",
    "    \n",
    "2. Each small dataframe will return a data quality report. These reports are aggregated, then returned.\n",
    "    * The recorded 'measurement_time' each partioned dataframe was measured will be slightly different (i.e. the time.now()), unless you have overridden this value, and we pick the latest measurement_time (the time the last Spark worker looked at the last bit of data)\n",
    "    * failed records are joined up, but the final report will limit to whatever is set globally (FAILED_RECORDS_IN_SAMPLE)\n",
    "\n",
    "3. Uniqueness rules are handled separately using native Spark code (as we can't check for uniqueness in samples of the data, we need knowledge of every value at once) - i.e. mapInPandas will not work for measuring uniqueness.\n",
    "4. The reason we use mapInPandas is because we can:\n",
    "    * re-use the majority of our codebase which makes it easier to maintain.\n",
    "    * Take advantage of all the flexibility in the pandas eval syntax for complex consistency rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:34.365282Z",
     "iopub.status.busy": "2025-09-18T12:52:34.365052Z",
     "iopub.status.idle": "2025-09-18T12:52:37.715844Z",
     "shell.execute_reply": "2025-09-18T12:52:37.714895Z"
    }
   },
   "outputs": [],
   "source": [
    "# overriding Spark's default partition size - here we split into 2 partitions\n",
    "dfs_2 = dfs.repartition(2)\n",
    "dq_report = dq_config.execute(dfs_2)\n",
    "dq_report.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Nested Data\n",
    "A lot of our data is nested. Many other data quality packages explicitly do not deal with nested data. This was one of the main motivations for writing our own package\n",
    "\n",
    "Let's get some simple nested data to work with\n",
    "\n",
    "### Example nested data - Pet Shop\n",
    "We will have data relating to customers, who own zero or more pets. Each pet can have one or more appointments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:37.728919Z",
     "iopub.status.busy": "2025-09-18T12:52:37.728648Z",
     "iopub.status.idle": "2025-09-18T12:52:37.759608Z",
     "shell.execute_reply": "2025-09-18T12:52:37.758920Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"customers\": {\n",
    "            \"name\": \"John\",\n",
    "            \"age\": 30,\n",
    "            \"pets\": [\n",
    "                {\n",
    "                    \"name\": \"Fido\",\n",
    "                    \"appointments\": [\n",
    "                        {\"date\": \"2022-01-01\", \"comment\": \"Fido First appointment\"},\n",
    "                        {\"date\": \"2022-01-02\", \"comment\": \"Fido Second appointment\"},\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Whiskers\",\n",
    "                    \"appointments\": [\n",
    "                        {\"date\": \"2022-02-03\", \"comment\": \"Whiskers First appointment\"},\n",
    "                        {\n",
    "                            \"date\": \"2022-02-04\",\n",
    "                            \"comment\": \"Whiskers Second appointment\",\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"customers\": {\n",
    "            \"name\": \"Jane\",\n",
    "            \"age\": 25,\n",
    "            \"pets\": [{\"name\": \"Rex\", \"appointments\": []}],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"customers\": {\n",
    "            \"name\": \"Mr No Pets\",\n",
    "            \"age\": 102,\n",
    "            \"pets\": [{\"name\": None, \"appointments\": []}],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"customers\": {\n",
    "            \"name\": \"Mrs Missing Pets\",\n",
    "            \"age\": 15,\n",
    "            \"pets\": [\n",
    "                {\"name\": \"missing\", \"appointments\": [{\"date\": None, \"comment\": \"none\"}]}\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\n",
    "            \"customers\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"name\", StringType(), True),\n",
    "                    StructField(\"age\", IntegerType(), True),  # <-- added age to schema\n",
    "                    StructField(\n",
    "                        \"pets\",\n",
    "                        ArrayType(\n",
    "                            StructType(\n",
    "                                [\n",
    "                                    StructField(\"name\", StringType(), True),\n",
    "                                    StructField(\n",
    "                                        \"appointments\",\n",
    "                                        ArrayType(\n",
    "                                            StructType(\n",
    "                                                [\n",
    "                                                    StructField(\n",
    "                                                        \"date\", StringType(), True\n",
    "                                                    ),\n",
    "                                                    StructField(\n",
    "                                                        \"comment\", StringType(), True\n",
    "                                                    ),\n",
    "                                                ]\n",
    "                                            )\n",
    "                                        ),\n",
    "                                        True,\n",
    "                                    ),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                        True,\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_pets = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:37.763910Z",
     "iopub.status.busy": "2025-09-18T12:52:37.763418Z",
     "iopub.status.idle": "2025-09-18T12:52:37.769345Z",
     "shell.execute_reply": "2025-09-18T12:52:37.768753Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:37.772058Z",
     "iopub.status.busy": "2025-09-18T12:52:37.771830Z",
     "iopub.status.idle": "2025-09-18T12:52:37.957178Z",
     "shell.execute_reply": "2025-09-18T12:52:37.956364Z"
    }
   },
   "outputs": [],
   "source": [
    "# At a high level we have two columns (id, and customers)\n",
    "# We need to bury deep into the customers column to pull out different values to test DQ rules against\n",
    "df_pets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we handle nested data\n",
    "In order for our existing data quality code to work with nested data, we first need to flatten the dataframe\n",
    "\n",
    "This is all done for you, but it's worthwhile seeing how this works.\n",
    "\n",
    "* under-the-hood, we use the flatten_spark function\n",
    "* I suggest you use this whilst creating your data quality rules, to check the dataframe is being flattened in the way you intend.\n",
    "    * Nested data can be quite complicated, especially if you are exploding multiple levels across multiple columns\n",
    "* Once flattened, we measure the data quality rules (it's treated like an ordinary dataframe)\n",
    "\n",
    "### How to refer to nested values\n",
    "We use notation similar to JSON path strings, if you are familiar with those.\n",
    "1. To access all pet names for our customers, we want to 'explode' the array of pets and pull out the name:\n",
    "\n",
    "```yaml\n",
    "field: customers.pets[*].name\n",
    "```\n",
    "\n",
    "The [*] value will return details for every pet\n",
    "\n",
    "2. Sometimes (e.g. for completeness measures), we just want to know if we have at least one pet name\n",
    "\n",
    "```yaml\n",
    "field: customers.pets[].name\n",
    "```\n",
    "\n",
    "This will pull the first non-null pet name for each customer (unless all of their pet names are null, in which case it will return null)\n",
    "\n",
    "3. We need to keep track of new column names as the dataframe is being sequentially 'exploded' and flattened. We therefore rename the columns of the newly created dataframe:\n",
    "    1. full-stops replaced with underscores\n",
    "    2. [*] replaced with _all (to signify we are extracting all values)\n",
    "    3. [] replaced with _first (to signify we pick the first non-null value)\n",
    "    ```yaml\n",
    "    customers.pets[*].name > customers_pets_all_name\n",
    "    customers.pets[].name > customers_pets_first_name\n",
    "4. let's use flatten_spark and see this in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:37.961098Z",
     "iopub.status.busy": "2025-09-18T12:52:37.960779Z",
     "iopub.status.idle": "2025-09-18T12:52:38.329135Z",
     "shell.execute_reply": "2025-09-18T12:52:38.328339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note how we will get two rows for John, as he has two pets.\n",
    "df_flat = flatten_spark(\n",
    "    df_pets, flatten_cols=[\"id\", \"customers.name\", \"customers.pets[*].name\"]\n",
    ")\n",
    "df_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:38.332531Z",
     "iopub.status.busy": "2025-09-18T12:52:38.332217Z",
     "iopub.status.idle": "2025-09-18T12:52:38.685940Z",
     "shell.execute_reply": "2025-09-18T12:52:38.683463Z"
    }
   },
   "outputs": [],
   "source": [
    "# If we pull just the first pet for each customer, we expect one row per customer\n",
    "# Note that as Spark is unordered, you may get a different 'first' pet each time you run this (likely for large data)\n",
    "df_flat = flatten_spark(\n",
    "    df_pets, flatten_cols=[\"id\", \"customers.name\", \"customers.pets[].name\"]\n",
    ")\n",
    "df_flat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data Quality Rules for Nested Data\n",
    "In your YAML configuration file:\n",
    "\n",
    "1. Just refer to the specific field using the nested format:\n",
    "```yaml\n",
    "field: customers.pets[].name\n",
    "function: completeness\n",
    "```\n",
    "\n",
    "2. For consistency rules, you MUST use backticks around EVERY column name (as this is how they are extracted)\n",
    "```yaml\n",
    "field: customers.pets[].name\n",
    "function: consistency\n",
    "expression: \"`customers.age` > 18\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:38.690602Z",
     "iopub.status.busy": "2025-09-18T12:52:38.690259Z",
     "iopub.status.idle": "2025-09-18T12:52:39.484318Z",
     "shell.execute_reply": "2025-09-18T12:52:39.483414Z"
    }
   },
   "outputs": [],
   "source": [
    "age_rule = ConsistencyRule(\n",
    "    field=\"customers.age\", expression=\"`customers.age` > 18\"\n",
    ")  # Note: ESSENTIAL use of backticks around the column expression customers.age\n",
    "\n",
    "# Before running the data quality rules on nested data, it's a good idea to check the dataframe being sent\n",
    "df_flat = flatten_spark(df_pets, flatten_cols=[\"customers.age\", \"id\"])\n",
    "print(\n",
    "    \"====== This is the DataFrame that will be sent (with the id field added for clarity)======\"\n",
    ")\n",
    "print(\"====== We create this with the 'flatten_spark' function ======\")\n",
    "df_flat.show()\n",
    "print(\"===================\")\n",
    "nested_result = age_rule.evaluate(df_pets)\n",
    "print(\"Data Quality Report DataFrame\")\n",
    "nested_result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:39.487980Z",
     "iopub.status.busy": "2025-09-18T12:52:39.487687Z",
     "iopub.status.idle": "2025-09-18T12:52:39.491309Z",
     "shell.execute_reply": "2025-09-18T12:52:39.490492Z"
    }
   },
   "outputs": [],
   "source": [
    "# One of our ages is less than 18 - note the change in column name (underscore between customers and age):\n",
    "nested_result.records_failed_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running from a Config File\n",
    "We provide an example config file (`nested_data_rules.yaml`) to show you a range of rules you can apply\n",
    "\n",
    "You can find this file in the tutorial directory (same location as this notebook):\n",
    "\n",
    "```yaml\n",
    "source_data: Nested Pet Shop Data\n",
    "measurement_time: 2025-01-01\n",
    "rules:\n",
    "- field: customers.age\n",
    "  function: validity_numerical_range\n",
    "  min_value: 18\n",
    "  max_value: 120\n",
    "- field: customers.pets[*].appointments[*].date\n",
    "  rule_description: All dates are no more than a year in the future from 1st Jan 2022\n",
    "  function: timeliness_static\n",
    "  start_date: 2022-01-01\n",
    "  end_date: 2023-01-01\n",
    "- field: customers.pets[].name\n",
    "  function: completeness\n",
    "- field: customers.name\n",
    "  function: consistency\n",
    "  rule_description: If the customer is a child they can't be a Mr or a Mrs\n",
    "  expression:\n",
    "    if: '`customers.age` < 18'\n",
    "    then: '~`customers.name`.str.startswith(\"Mr\")'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:39.496167Z",
     "iopub.status.busy": "2025-09-18T12:52:39.495792Z",
     "iopub.status.idle": "2025-09-18T12:52:39.512863Z",
     "shell.execute_reply": "2025-09-18T12:52:39.512068Z"
    }
   },
   "outputs": [],
   "source": [
    "nested_config2 = DataQualityConfig.from_yaml(\n",
    "    \"resources/nested_data_rules.yaml\", regex_yaml_path=\"resources/regex_patterns.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_config2.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:39.516808Z",
     "iopub.status.busy": "2025-09-18T12:52:39.516400Z",
     "iopub.status.idle": "2025-09-18T12:52:43.086967Z",
     "shell.execute_reply": "2025-09-18T12:52:43.086166Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_pets_nested_report = nested_config2.execute(df_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:43.090406Z",
     "iopub.status.busy": "2025-09-18T12:52:43.090144Z",
     "iopub.status.idle": "2025-09-18T12:52:43.105446Z",
     "shell.execute_reply": "2025-09-18T12:52:43.104632Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_pets_nested_report.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:43.108932Z",
     "iopub.status.busy": "2025-09-18T12:52:43.108682Z",
     "iopub.status.idle": "2025-09-18T12:52:44.059086Z",
     "shell.execute_reply": "2025-09-18T12:52:44.058355Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark GOTCHAs\n",
    "\n",
    "A few limitations to note:\n",
    "\n",
    "**Auto YAML creation**\n",
    "\n",
    "Because of the column name changes (customers[*].age > customers_all_age) you can't re-create the exact same DataQualityConfig rules from the data quality report (the column names have now changed) but this is fairly quick to remedy in a code editor with find and replace.\n",
    "\n",
    "```python\n",
    "config = DataQualityConfig.from_report(DQ_Report)\n",
    "```\n",
    "i.e. replace '_all' with [*], '_first' with [], and replace single '_' with '.' in the resulting config to get back to the original field names.\n",
    "\n",
    "**Consistency Rules**\n",
    "Be careful with the expressions you run in consistency checks. If you rely on dataframe-wide statistics for an expressions, this will be unreliable if the dataframe is partitioned. e.g. `col1 <= other_col.mean()` - the value of other_col.mean() will vary depending on how the dataframe has been partitioned. If running this check is essential, you will have to push the data to a single spark worker using  `df_spark.repartition(1)` or pre-process the data so the mean value is in it's own column (so the same value goes to every partition)\n",
    "\n",
    "**Timezones**\n",
    "\n",
    " I've found Spark doesn't like timezone-naive datetimes, and will stamp a timezone on a date column (depending on how you've configured Spark, this could be local time, and so not always UTC). I suggest you control this explicitly in your source data, and convert everything to UTC, so that the time rules work as intended.\n",
    "\n",
    "**Approximations**\n",
    "\n",
    "Especially with small bits of test data being split across parititions. Spark may introduce some rounding errors - insignicant from a data quality measure point of view, but odd to see. Our unit testing has to account for this by checking values are equal with an error of 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:52:44.063337Z",
     "iopub.status.busy": "2025-09-18T12:52:44.063076Z",
     "iopub.status.idle": "2025-09-18T12:52:44.068786Z",
     "shell.execute_reply": "2025-09-18T12:52:44.067885Z"
    }
   },
   "outputs": [],
   "source": [
    "DataQualityConfig.from_report(dq_pets_nested_report).model_dump()\n",
    "\n",
    "# Note how the 'field' name in the output below has the new column names such as 'customers_age'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production!\n",
    "\n",
    "Now you can deploy at scale against Spark dataframes you should be almost ready for production :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchq-data-quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
