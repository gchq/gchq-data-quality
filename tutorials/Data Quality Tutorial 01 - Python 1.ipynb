{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Tutorial - Python 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisities\n",
    "You should have worked through the Data Quality Core training module which is a foundation for all data quality training. This will explain the Data Quality Dimensions (like Uniqueness), and how each measure is calculated.\n",
    "\n",
    "For example, you should already be able to explain:\n",
    "* the benefits of measuring data quality\n",
    "* what each of the 6 DAMA Dimensions are and what they mean\n",
    "* what a completeness score of 0.5 means\n",
    "* how multiple scores are combined to give an overall data quality index\n",
    "\n",
    "### Prior coding experience required\n",
    "* You should know what pandas dataframes are\n",
    "* You should have some basic python knowledge\n",
    "\n",
    "## Aims of Python 1\n",
    "* To be able to measure data quality on dataframes\n",
    "* To use and understand all 8 data quality functions in the code\n",
    "* To export the results to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:42.559268Z",
     "iopub.status.busy": "2025-09-18T12:51:42.558938Z",
     "iopub.status.idle": "2025-09-18T12:51:44.481803Z",
     "shell.execute_reply": "2025-09-18T12:51:44.481147Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# we import the rules, the data quality configuration and final report\n",
    "from gchq_data_quality import (\n",
    "    AccuracyRule,\n",
    "    CompletenessRule,\n",
    "    ConsistencyRule,\n",
    "    DataQualityConfig,\n",
    "    DataQualityReport,\n",
    "    TimelinessRelativeRule,\n",
    "    TimelinessStaticRule,\n",
    "    UniquenessRule,\n",
    "    ValidityNumericalRangeRule,\n",
    "    ValidityRegexRule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple dataframe with known data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.485066Z",
     "iopub.status.busy": "2025-09-18T12:51:44.484770Z",
     "iopub.status.idle": "2025-09-18T12:51:44.508886Z",
     "shell.execute_reply": "2025-09-18T12:51:44.508275Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2, 3, 3, 5],  # 4 /5 unique\n",
    "        \"name\": [\"John\", \"Jane\", \"Dave\", None, \"Missing\"],  # 1 null value\n",
    "        \"age\": [30, 25, 102, 15, -5],  # a negative age\n",
    "        \"email\": [\n",
    "            \"john@example.com\",\n",
    "            \"jane@example.com\",\n",
    "            \"dave@example\",\n",
    "            \"test@test.com\",\n",
    "            \"alice@example.com\",\n",
    "        ],  # invalid 3rd email\n",
    "        \"category\": [\"A\", \"B\", \"C\", \"D\", \"X\"],\n",
    "        \"score\": [\n",
    "            10,\n",
    "            20,\n",
    "            30,\n",
    "            40,\n",
    "            -1,\n",
    "        ],  # missing scores are defined as -1\n",
    "        \"date\": [\n",
    "            datetime(2023, 1, 1),\n",
    "            datetime(2023, 2, 1),\n",
    "            datetime(2023, 3, 1),\n",
    "            datetime(2021, 1, 1),  # one date too old\n",
    "            datetime(2023, 5, 1),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a few custom classes to help enforce data validation on our data quality results, and provide some useful functionality which you will see later in Python 2, such as being able to generate a list of data quality rules you have applied (to save you typing them all out)\n",
    "\n",
    "We use a package called Pydantic for these classes, which you may want to investigate (but this is not necessary for this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.565919Z",
     "iopub.status.busy": "2025-09-18T12:51:44.565537Z",
     "iopub.status.idle": "2025-09-18T12:51:44.569253Z",
     "shell.execute_reply": "2025-09-18T12:51:44.568477Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT = DataQualityReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a single rule by first defining it, then evaluating it against our dataframe\n",
    "### Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.572204Z",
     "iopub.status.busy": "2025-09-18T12:51:44.571969Z",
     "iopub.status.idle": "2025-09-18T12:51:44.577005Z",
     "shell.execute_reply": "2025-09-18T12:51:44.576233Z"
    }
   },
   "outputs": [],
   "source": [
    "uniqueness_rule = UniquenessRule(field=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the details of this rule definition with to_dict() (for Pydantic users, this is equivalent to model_dump())\n",
    "Where you can see the default values that have been assigned, such as:\n",
    "* data_quality_dimensions - it's 'Uniqueness'\n",
    "* rule_description - it's None as we haven't provided it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the rule like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_result = uniqueness_rule.evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.580378Z",
     "iopub.status.busy": "2025-09-18T12:51:44.580075Z",
     "iopub.status.idle": "2025-09-18T12:51:44.585620Z",
     "shell.execute_reply": "2025-09-18T12:51:44.584703Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_result.model_dump(mode=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the output\n",
    "\n",
    "#### Key data quality outputs\n",
    "* The field - we looked at the 'id' column\n",
    "* The pass rate - 0.8 (80%)\n",
    "* The number of records we checked (records_evaluated) - 5\n",
    "    * We can back-calculate the number of records that passed (0.8 * 5 = 4)\n",
    "* The data quality dimension - Uniqueness\n",
    "\n",
    "#### Diagnostic outputs\n",
    "* The records failed sample will provide us values that failed\n",
    "* The records failed IDs will tell us where they are in the dataframe (1st row is 0)\n",
    "\n",
    "#### Additional observations\n",
    "* You will notice that it has set some default values for you (such as the measurement_time, which defaults to 'now' in UTC)\n",
    "* You may also notice that the rule_data has defaulted to a dictionary string containing all the parameters you entered, such that you can recreate this data quality rule from the output (covered in more detail in Python 2)\n",
    "\n",
    "### Accessing values\n",
    "You can access values as attributes of the result i.e. dq_result.name_of_variable (e.g. dq_result.pass_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.588919Z",
     "iopub.status.busy": "2025-09-18T12:51:44.588684Z",
     "iopub.status.idle": "2025-09-18T12:51:44.593311Z",
     "shell.execute_reply": "2025-09-18T12:51:44.592540Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    rf\" 4 \\ 5 ids are actually unique, giving a data quality score of {dq_result.pass_rate}\"\n",
    ")\n",
    "print(f\"we can track the rows that are incorrect: {dq_result.records_failed_ids} \")\n",
    "print(\n",
    "    f\"And show a sample of the values that failed the test : {dq_result.records_failed_sample}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.598392Z",
     "iopub.status.busy": "2025-09-18T12:51:44.597984Z",
     "iopub.status.idle": "2025-09-18T12:51:44.615047Z",
     "shell.execute_reply": "2025-09-18T12:51:44.614037Z"
    }
   },
   "outputs": [],
   "source": [
    "# The records attribute of our DataQualityReport is a list, so we just append the score to it\n",
    "FINAL_REPORT.results.append(dq_result)\n",
    "\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "Checks for the number of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.618739Z",
     "iopub.status.busy": "2025-09-18T12:51:44.618503Z",
     "iopub.status.idle": "2025-09-18T12:51:44.623788Z",
     "shell.execute_reply": "2025-09-18T12:51:44.623086Z"
    }
   },
   "outputs": [],
   "source": [
    "completeness_rule = CompletenessRule(field=\"name\")\n",
    "completeness_result = completeness_rule.evaluate(df)\n",
    "completeness_result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if data has otherways of meaning a values is incomplete, like the word 'missing'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.626841Z",
     "iopub.status.busy": "2025-09-18T12:51:44.626570Z",
     "iopub.status.idle": "2025-09-18T12:51:44.633243Z",
     "shell.execute_reply": "2025-09-18T12:51:44.632441Z"
    }
   },
   "outputs": [],
   "source": [
    "# all functions have na_values as a parameter, this can be a string (if one item) or a list\n",
    "completeness_rule.na_values = \"Missing\"  # or [\"Missing\", \"NA\"]\n",
    "\n",
    "# if 'Missing' signifies a NULL value in 'name' column, then we skip it\n",
    "# our pass rate is therefore lower\n",
    "completeness_result = completeness_rule.evaluate(df)\n",
    "print(\n",
    "    \"Now we have added 'Missing' to be the same as NULL, our pass rate is lower:\",\n",
    "    completeness_result.pass_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.636097Z",
     "iopub.status.busy": "2025-09-18T12:51:44.635848Z",
     "iopub.status.idle": "2025-09-18T12:51:44.652135Z",
     "shell.execute_reply": "2025-09-18T12:51:44.651159Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.append(completeness_result)\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "This is the hardest dimension to measure within the data itself as really it should be cross-checked with an authoritative source\n",
    "E.g. checking someone's date of birth with the actual birth certificate\n",
    "\n",
    "For this package, we do an accuracy check by verifying the values in a column are drawn from an authoritative list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.656042Z",
     "iopub.status.busy": "2025-09-18T12:51:44.655661Z",
     "iopub.status.idle": "2025-09-18T12:51:44.665119Z",
     "shell.execute_reply": "2025-09-18T12:51:44.664145Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_rule = AccuracyRule(field=\"category\", valid_values=[\"A\", \"B\", \"C\", \"D\"])\n",
    "accuracy_result = accuracy_rule.evaluate(df)\n",
    "accuracy_result.to_dict()[\"records_failed_sample\"]\n",
    "\n",
    "# here 'X' must be inaccurate as it is not one of the allowed values of A,B,C,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had values we DIDN'T want to be present, we would set inverse to be 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.669249Z",
     "iopub.status.busy": "2025-09-18T12:51:44.669011Z",
     "iopub.status.idle": "2025-09-18T12:51:44.674213Z",
     "shell.execute_reply": "2025-09-18T12:51:44.673169Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will get the opposite result now (1/5 values will pass)\n",
    "accuracy_rule.inverse = True\n",
    "accuracy_rule.evaluate(df).to_dict()[\"records_failed_sample\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring NULL values.\n",
    "\n",
    "It's more insightful to compare rules together with completeness data, so rules like accuracy by default will\n",
    "ignore NULL values. As the question we are asking is: of all the records we are checking (that have values in them), which are accurate?\n",
    "\n",
    "If we count NULL as 'inaccurate', then columns with large numbers of NULL values in them would dominate the pass rate.\n",
    "To take an extreme example, a column with 1,000,000 rows, with most NULL, the pass rate will always be 0.0000 (to 4.d.p) irrespecitve of \n",
    "how accurate the small number of values actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.677109Z",
     "iopub.status.busy": "2025-09-18T12:51:44.676884Z",
     "iopub.status.idle": "2025-09-18T12:51:44.683857Z",
     "shell.execute_reply": "2025-09-18T12:51:44.683007Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"We evaluate all the records that are not null: {accuracy_result.records_evaluated} records checked\"\n",
    ")\n",
    "print(\"With 'X' being swapped for NULL..\")\n",
    "accuracy_rule.na_values = [\"X\"]\n",
    "print(f\"We evaluate fewer records: {accuracy_rule.evaluate(df).records_evaluated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can bypass this behaviour, but we do not recommend it (other than in the ConsistencyRule), as it changes the meaning of the rule, and could\n",
    "# cause confusion if comparing your data between teams\n",
    "accuracy_rule.skip_if_null = \"never\"\n",
    "accuracy_rule.evaluate(df).records_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.688305Z",
     "iopub.status.busy": "2025-09-18T12:51:44.688060Z",
     "iopub.status.idle": "2025-09-18T12:51:44.703906Z",
     "shell.execute_reply": "2025-09-18T12:51:44.703165Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.append(accuracy_result)\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency\n",
    "A logical internal check on the data, such as a date of birth being earlier than a date of death, or some other mathematical check on a single column.\n",
    "Under the hood we use pandas 'eval' syntax which is quite flexible, this is the same syntax as used in pandas 'query' syntax e.g. df.query('my_column > 3')\n",
    "\n",
    "#### BACKTICKS\n",
    "You MUST use backticks (`) around each column name - this is how we extract the columns in our code for collecting suitable samples of output data. In pandas.eval(), backticks are only usually required if your column names have spaces in them, but in our package you must use them at all times.\n",
    "\n",
    "#### Deciding on the 'field'\n",
    "Even though consistency checks can operate across many columns at once, you still need to specify a 'field' parameter, pick the one that makes the most sense\n",
    "\n",
    "#### Eval help pages\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.eval.html#pandas.eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.706768Z",
     "iopub.status.busy": "2025-09-18T12:51:44.706537Z",
     "iopub.status.idle": "2025-09-18T12:51:44.720487Z",
     "shell.execute_reply": "2025-09-18T12:51:44.719505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple expressions are a single statement\n",
    "consistency_rule = ConsistencyRule(field=\"age\", expression=\"`age` > 3\")\n",
    "consistency_result = consistency_rule.evaluate(df)\n",
    "consistency_result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.723396Z",
     "iopub.status.busy": "2025-09-18T12:51:44.723168Z",
     "iopub.status.idle": "2025-09-18T12:51:44.734660Z",
     "shell.execute_reply": "2025-09-18T12:51:44.733959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Complex expressions are a dictionary with keys of 'if' and 'then'\n",
    "consistency_rule2 = ConsistencyRule(\n",
    "    field=\"age\", expression={\"if\": \"`age` > 3\", \"then\": \"`score` <= 40\"}\n",
    ")\n",
    "consistency_result2 = consistency_rule2.evaluate(df)\n",
    "print(\"We have constrained our analysis using an 'if' in the expression.\")\n",
    "print(\n",
    "    f\"We are evaluating fewer records (only those that meet the 'if' criteria): {consistency_result2.records_evaluated}\"\n",
    ")\n",
    "print(\n",
    "    f\"Our pass rate is now higher, even though the expression age > 3 is the same: {consistency_result2.pass_rate=}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Nulls with `skip_if_null`\n",
    "\n",
    "The `skip_if_null` parameter controls how null (missing) values are handled during consistency checks. You can then decide if one column in a comparison being NULL makes the record inconsitent or not (or should it be ignored / skipped)\n",
    "\n",
    "- `\"any\"`: Skip (exclude) rows where **any** relevant column is null.\n",
    "- `\"all\"`: Skip rows only if **all** relevant columns are null (default behaviour).\n",
    "- `\"never\"`: Include all rows, even those with nulls; nulls are passed into the expression.\n",
    "\n",
    "You can apply this value to any rule, although we recommend only using it with the ConsistencyRule.\n",
    "\n",
    "You cannot override it in the Completeness rule (it makes no sense to skip null values in a rule that is counting NULLs!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.737933Z",
     "iopub.status.busy": "2025-09-18T12:51:44.737623Z",
     "iopub.status.idle": "2025-09-18T12:51:44.746794Z",
     "shell.execute_reply": "2025-09-18T12:51:44.745871Z"
    }
   },
   "outputs": [],
   "source": [
    "# IF we set the -5 value to be NULL and skip it... all our records pass\n",
    "consistency_rule.na_values = -5\n",
    "print(\n",
    "    f\"By setting -5 age as NULL, all our records now pass: {consistency_rule.evaluate(df).pass_rate}\"\n",
    ")\n",
    "print(\n",
    "    f\"Because we are only evaluating non-null values: {consistency_rule.evaluate(df).records_evaluated}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.749563Z",
     "iopub.status.busy": "2025-09-18T12:51:44.749332Z",
     "iopub.status.idle": "2025-09-18T12:51:44.757967Z",
     "shell.execute_reply": "2025-09-18T12:51:44.757284Z"
    }
   },
   "outputs": [],
   "source": [
    "# By passing in 'never' we pass NULL into the expression (and it fails because NULL is not bigger than 3, so we get a lower DQ score)\n",
    "consistency_rule_never_skip = (\n",
    "    consistency_rule.model_copy()\n",
    ")  # just to avoid modifying the original\n",
    "consistency_rule_never_skip.skip_if_null = \"never\"\n",
    "never_skip_result = consistency_rule_never_skip.evaluate(df)\n",
    "\n",
    "print(\n",
    "    f\"We now evalute more rules:\\n{never_skip_result.records_evaluated=}\\nand so have a lower pass rate:\\n{never_skip_result.pass_rate=} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pandas Eval Syntax Examples\n",
    "\n",
    "\n",
    "Below are increasingly complex examples of part of expressions you might put in the `expression` parameter for the `consistency` function, showing how to use the pandas eval syntax. Note that we can use mathematical and boolean operators, and access pandas series methods using .str (string methods), .dt (datetime methods) - of which there are many. Some of these might not be actual consistency expressions, per se, but they give you an idea of what is achievable. You can then combine this functionality to create quite complex expressions.\n",
    "\n",
    "You final consistency expressions MUST result in a boolean output (true or false)\n",
    "\n",
    "Use BACKTICKS (`) around ALL column names:\n",
    "\n",
    "```python\n",
    "\"`Age Column` > 0\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Basic numeric comparison**\n",
    "```python\n",
    "\"`age` > 0\"\n",
    "```\n",
    "Checks that all ages are positive numbers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Simple equality**\n",
    "```python\n",
    "\"`category` == 'A'\"\n",
    "```\n",
    "Ensures the `category` column is 'A'.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **String methods**\n",
    "```python\n",
    "\"`name`.str.istitle()\"\n",
    "```\n",
    "Confirms that names are written in title case.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Date comparison**\n",
    "```python\n",
    "\"`date` > '2022-01-01'\"\n",
    "```\n",
    "Ensures all dates are after January 1st, 2022.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Numeric relationship between columns**\n",
    "```python\n",
    "\"`score` < `age`\"\n",
    "```\n",
    "Score must always be less than the age.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Multiple conditions using boolean operators**\n",
    "```python\n",
    "\"(`age` >= 18) & (`category`.isin(['A', 'B', 'C']))\"\n",
    "```\n",
    "Only adult records in certain categories.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **String and numeric combined**\n",
    "```python\n",
    "\"`email`.str.contains('@example.com') & (`score` > 10)\"\n",
    "```\n",
    "Email must be company domain, and score must be above 10.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Conditional logic with a dict expression**\n",
    "```python\n",
    "{\"if\": \"`age` < 18\", \"then\": \"`category` == 'D'\"}\n",
    "```\n",
    "If age is less than 18, category must be 'D'.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Datetime attribute with comparison**\n",
    "```python\n",
    "\"`date`.dt.year == 2023\"\n",
    "```\n",
    "All dates should be in the year 2023.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Multiple columns & string method combined**\n",
    "```python\n",
    "\"`name`.notnull() & `email`.str.endswith('@example.com') & (`score` >= 20)\"\n",
    "```\n",
    "Name must not be null, email must end with '@example.com', and score is at least 20.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Complex if-then logic referencing 3 columns**\n",
    "This is the sort of example where using skip_if_null is useful, is a record inconsistent if score is NULL for example? Or should you ignore that (skip_if_null = 'Any')\n",
    "```python\n",
    "{\n",
    "  \"if\": \"(`score` < `age`) & `email`.str.contains('@example.com')\",\n",
    "  \"then\": \"`name`.str.startswith('J')\"\n",
    "}\n",
    "```\n",
    "If score is less than age and email is a company email, the name must start with 'J'.\n",
    "\n",
    "---\n",
    "\n",
    "### Consistency Syntax Examples â€” Numeric Distribution Focus\n",
    "\n",
    "#### 12. **Column mean within a range**\n",
    "```python\n",
    "\"abs(`score`.mean() - 50) < 5\"\n",
    "```\n",
    "Checks if the mean score is within 5 points of 50.\n",
    "\n",
    "---\n",
    "\n",
    "#### 13. **Standard deviation not too high**\n",
    "```python\n",
    "\"`score`.std() < 15\"\n",
    "```\n",
    "Ensures the standard deviation of the `score` column is below 15.\n",
    "\n",
    "---\n",
    "\n",
    "#### 14. **No extreme outliers (values within 3 standard deviations)**\n",
    "```python\n",
    "'abs(`score` - `score`.mean()) <= (3* `score`.std())'\n",
    "```\n",
    "All scores must be within 3 standard deviations from the mean.\n",
    "\n",
    "---\n",
    "\n",
    "#### 15. **Median (50th percentile) check**\n",
    "```python\n",
    "\"`score`.median() > 60\"\n",
    "```\n",
    "The median score must be above 60.\n",
    "\n",
    "---\n",
    "\n",
    "#### 16. **Quantile-based (90% below threshold)**\n",
    "```python\n",
    "\"`score`.quantile(0.9) < 90\"\n",
    "```\n",
    "Ninety percent of scores must be below 90.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.761550Z",
     "iopub.status.busy": "2025-09-18T12:51:44.761231Z",
     "iopub.status.idle": "2025-09-18T12:51:44.777115Z",
     "shell.execute_reply": "2025-09-18T12:51:44.776309Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.append(consistency_result)\n",
    "FINAL_REPORT.results.append(consistency_result2)\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness\n",
    "\n",
    "#### Introduction\n",
    "The aim of timeliness as a measure is to check a date falls within a certain range (i.e. later than 'start' and earlier than 'end').\n",
    "\n",
    "We have two flavours of this function:\n",
    "* timeliness_static (fixed start and end dates)\n",
    "* timeliness_relative (relative start and end dates)\n",
    "\n",
    "Requiring this flexibilty of date comparisons was one of the main reasons for writing our own package. It's not so simple as finding one date is earlier than another, we often require a date to fall within a certain window.\n",
    "\n",
    "#### A note on 'time'\n",
    "There are plenty of ways to get time calculations wrong, given time zones and British Summer Time.\n",
    "Within the function we normalise everything to UTC. If the dates you are measuring are not written in ISO 8601 standard with timezone information or a UTC offset, then we assume it's UTC and raise a warning. \n",
    "\n",
    "If none of your times have timezone information then this won't matter (as everything will just change to UTC), but it does matter when stamping values like 'datetime.now()' which will return local time.\n",
    "\n",
    "For best results, pre-convert your date values to UTC, and if using 'now' as a reference point use datetime.now(timezone.utc).\n",
    "\n",
    "#### Timeliness Static\n",
    "Used with an absolute (static) start and end date. The comparison is <= and >= (it's inclusive of start and end date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.781120Z",
     "iopub.status.busy": "2025-09-18T12:51:44.780828Z",
     "iopub.status.idle": "2025-09-18T12:51:44.799167Z",
     "shell.execute_reply": "2025-09-18T12:51:44.798468Z"
    }
   },
   "outputs": [],
   "source": [
    "# we have one value earlier than 2023-01-01 (score should be 0.8)\n",
    "# note how you can pass in a datetime string or a datetime object\n",
    "\n",
    "timeliness_static_rule = TimelinessStaticRule(\n",
    "    field=\"date\", start_date=\"2023-01-01\", end_date=datetime(2023, 6, 1)\n",
    ")\n",
    "timeliness_static_result = timeliness_static_rule.evaluate(df)\n",
    "timeliness_static_result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have convereted start_date (and end_date) to timezone aware values in UTC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeliness_static_rule.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Leaving start or end date as None\n",
    "If start_date is None, then we don't check against it, we just check the date is no later than end_date\n",
    "\n",
    "If end_date is None, we just check the date is no earlier than start_date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.802651Z",
     "iopub.status.busy": "2025-09-18T12:51:44.802349Z",
     "iopub.status.idle": "2025-09-18T12:51:44.808228Z",
     "shell.execute_reply": "2025-09-18T12:51:44.807662Z"
    }
   },
   "outputs": [],
   "source": [
    "# We just check times are no later than 1 June 2023, so this will pass the date in 2021 (all records now pass)\n",
    "timeliness_static_rule.start_date = None\n",
    "timeliness_static_rule.evaluate(df).pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.811144Z",
     "iopub.status.busy": "2025-09-18T12:51:44.810866Z",
     "iopub.status.idle": "2025-09-18T12:51:44.825369Z",
     "shell.execute_reply": "2025-09-18T12:51:44.824651Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.append(timeliness_static_result)\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeliness Relative\n",
    "This is when you want a more dynamic check. You want to know your date is within a start and end period, relative to another date (a reference date). For example, you might want to ensure the booking date is in the future relative to the order date.\n",
    "\n",
    "We allow for the reference date being 'now' (the default, in UTC of course) or a datetime object. Alternatively you can specify a reference_column - this will then compare each row of your dateframe and use the date values in that column as the reference.\n",
    "\n",
    "We use timedeltas to define the window relative to the reference date. Negative means past, positive means future.\n",
    "\n",
    "So setting start_timedelta='-5d' and end_timedelta='+6h' means your date should be within a time window of 5 days before and 6 hours after your reference date.\n",
    "\n",
    "We stick to pandas timedelta formatting, so you can pass in \n",
    "* timedelta objects (timedelta(days=5))\n",
    "* a compatible string ('5d')\n",
    "* an ISO 8601 time duration string ('P5D') - just not using year (Y) or month (M) markers\n",
    "\n",
    "##### Valid timedelta periods\n",
    "Note that 'month' and 'year' are ambiguous due to leap years and variable numbers of days in months, so these are not valid for time durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.828244Z",
     "iopub.status.busy": "2025-09-18T12:51:44.827985Z",
     "iopub.status.idle": "2025-09-18T12:51:44.837834Z",
     "shell.execute_reply": "2025-09-18T12:51:44.837218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use '0d' as a start_timedelta amount to mean 'equal to or later than the reference_date'\n",
    "# Noting that if you don't specify a time in your reference date, then the default time of 00:00hrs will be used\n",
    "\n",
    "# Here we are checking if the 'date' is within 2 years of the future of reference_date (1st Jan) - one of our valies was 2021, so we should have 1 failing record out of 5, (score of 0.8)\n",
    "\n",
    "timeliness_relative_rule = TimelinessRelativeRule(\n",
    "    field=\"date\",\n",
    "    reference_date=\"2023-01-01\",\n",
    "    start_timedelta=0,\n",
    "    end_timedelta=timedelta(days=365 * 2),\n",
    ")\n",
    "timelienss_relative_result = timeliness_relative_rule.evaluate(df)\n",
    "timelienss_relative_result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have a column containing dates we want to compare on a per-row basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.840704Z",
     "iopub.status.busy": "2025-09-18T12:51:44.840480Z",
     "iopub.status.idle": "2025-09-18T12:51:44.849681Z",
     "shell.execute_reply": "2025-09-18T12:51:44.848751Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"reference_date_column\"] = df[\"date\"] + timedelta(days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just be careful on the logic...as we are comparing 'date' to a reference date that is now 3 days in the future for each date,\n",
    "our dates are all in the past relative to the reference column dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeliness_relative_rule2 = TimelinessRelativeRule(\n",
    "    field=\"date\",\n",
    "    reference_column=\"reference_date_column\",\n",
    "    start_timedelta=\"-3d\",\n",
    "    end_timedelta=\"0d\",\n",
    ")\n",
    "timeliness_relative_result2 = timeliness_relative_rule2.evaluate(df)\n",
    "timeliness_relative_result2.pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.append(timelienss_relative_result)\n",
    "FINAL_REPORT.results.append(timeliness_relative_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity\n",
    "\n",
    "We use validity to mean one of two things:\n",
    "* matching a Regular Expression (regex) pattern (e.g. just numbers [0-9]+)\n",
    "    * if you are unfamiliar with regular expressions, there is plenty of support online (YouTube tutorials, regex calculators etc)\n",
    "* falling within a numerical range (e.g. age should be >=1 and <=120)\n",
    "\n",
    "We skip over any NULL values. This is a deliberate design choice, as you can use a validity measure in conjunction with a completeness measure to get a full picture of your data. If we said that NULL values were invalid (or valid) we'd distort the picture, it would be difficult to work out why your data was scoring the way it was.\n",
    "\n",
    "As an example: imagine you have 1 million email records, and they are all null apart from 10 (but those 10 are all valid email addresses). If you included NULL in the valididy meausure, then the score would be 0.0000, as most records are NULL.\n",
    "However, comparing with a compeleteness score, we can interpret, that 1) we hardly have any email records but 2) of those we have, they are all valid.\n",
    "\n",
    "#### Validity Regex\n",
    "We pass in a regular expression and check the % of records that matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.853109Z",
     "iopub.status.busy": "2025-09-18T12:51:44.852757Z",
     "iopub.status.idle": "2025-09-18T12:51:44.860105Z",
     "shell.execute_reply": "2025-09-18T12:51:44.859311Z"
    }
   },
   "outputs": [],
   "source": [
    "validity_regex_rule = ValidityRegexRule(\n",
    "    field=\"email\", regex_pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    ")\n",
    "validity_regex_result = validity_regex_rule.evaluate(df)\n",
    "validity_regex_result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validity Numerical Range\n",
    "\n",
    "The check is inclusive (greater or equal to min_value, less than or equal to max_value)\n",
    "\n",
    "If you leave one as None it will get replaced by negative infinity (min_value) or infinity (max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.863490Z",
     "iopub.status.busy": "2025-09-18T12:51:44.863212Z",
     "iopub.status.idle": "2025-09-18T12:51:44.868938Z",
     "shell.execute_reply": "2025-09-18T12:51:44.868310Z"
    }
   },
   "outputs": [],
   "source": [
    "validity_numerical_range_rule = ValidityNumericalRangeRule(\n",
    "    field=\"age\", min_value=1, max_value=120\n",
    ")\n",
    "validity_numerical_range_result = validity_numerical_range_rule.evaluate(df)\n",
    "validity_numerical_range_result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_no_min = ValidityNumericalRangeRule(field=\"age\", max_value=150)\n",
    "rule_no_min.min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.871826Z",
     "iopub.status.busy": "2025-09-18T12:51:44.871567Z",
     "iopub.status.idle": "2025-09-18T12:51:44.876622Z",
     "shell.execute_reply": "2025-09-18T12:51:44.875920Z"
    }
   },
   "outputs": [],
   "source": [
    "# ignoring min_value - so negative ages will count as valid\n",
    "rule_no_min.evaluate(df).pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.879721Z",
     "iopub.status.busy": "2025-09-18T12:51:44.879441Z",
     "iopub.status.idle": "2025-09-18T12:51:44.895267Z",
     "shell.execute_reply": "2025-09-18T12:51:44.894131Z"
    }
   },
   "outputs": [],
   "source": [
    "FINAL_REPORT.results.extend([validity_regex_result, validity_numerical_range_result])\n",
    "FINAL_REPORT.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Tweaks to your Rules\n",
    "Now you have your final Data Quality Report! Congratulations on getting this far. \n",
    "\n",
    "### Additional details on your rules\n",
    "* The rule outputs a rule_data field - this contains all the information required to reconstruct the rule\n",
    "    *... more on that magic feature in the Python 2 tutorial\n",
    "* You can provide a rule ID (if you are logging or referencing your rules somewhere - this has no functionality in the code, it's just there in case you need it)\n",
    "\n",
    "### Add details of what you are measuring\n",
    "\n",
    "It would be useful to note down, within each rule, various things about the data you are measuring - the definitions of these terms were covered in the Data Quality Core PowerPoint:\n",
    "* measurment_time (this defaults to 'now', but you can overwrite it)\n",
    "* dataset name\n",
    "* dataset ID\n",
    "* measurement_sample\n",
    "* lifecycle stage\n",
    "    * make this sort alphabetically (if you take multiple measurements), so when you plot the data in a dashboard you can sort the data quality measures logically as they progress along the data lifecycle (e.g. 01 - ingest, 02 - enrich)\n",
    "\n",
    "This information is contained in a DataQualityConfig object, which can contain a list of all of your rules\n",
    "\n",
    "## Data Quality Config\n",
    "To conduct an evaluation across many rules at a point in time, you should define a DataQualityConfig object - this captures the above details (dataset_name, dataset_id etc.)\n",
    "\n",
    "A configuration is *executed* against a data source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.898489Z",
     "iopub.status.busy": "2025-09-18T12:51:44.898256Z",
     "iopub.status.idle": "2025-09-18T12:51:44.903809Z",
     "shell.execute_reply": "2025-09-18T12:51:44.902955Z"
    }
   },
   "outputs": [],
   "source": [
    "dq_config = DataQualityConfig(\n",
    "    dataset_name=\"Tutorial Data\",\n",
    "    lifecycle_stage=\"02 Post Processing\",\n",
    "    measurement_time=\"2025-01-01\",\n",
    "    rules=[\n",
    "        uniqueness_rule,\n",
    "        completeness_rule,\n",
    "        accuracy_rule,\n",
    "        timeliness_relative_rule,\n",
    "        timeliness_static_rule,\n",
    "        consistency_rule,\n",
    "        validity_numerical_range_rule,\n",
    "        validity_regex_rule,\n",
    "    ],\n",
    ")\n",
    "\n",
    "dq_report = dq_config.execute(df)\n",
    "dq_report.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting your final report\n",
    "You can export the report to a dataframe / dictionary / JSON and then \n",
    "use existing pandas functionality to save to a CSV or Excel file.\n",
    "\n",
    "You can modify the appearance of your dataframe (see the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.907181Z",
     "iopub.status.busy": "2025-09-18T12:51:44.906936Z",
     "iopub.status.idle": "2025-09-18T12:51:44.912244Z",
     "shell.execute_reply": "2025-09-18T12:51:44.911605Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can how export your report to a dataframe and share the results as a CSV file\n",
    "# We can modify the measurement_time format and adjust the decimal place accuracy (to make it more readable)\n",
    "# If you want to align the invalid row numbers with the row number in Excel, you can shift it by 2. (Excel data starts at 2, not 0)\n",
    "df_report = dq_report.to_dataframe(\n",
    "    decimals=2, measurement_time_format=\"%Y-%m-%d %H:%M\", records_failed_ids_shift=2\n",
    ")\n",
    "df_report.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T12:51:44.915012Z",
     "iopub.status.busy": "2025-09-18T12:51:44.914791Z",
     "iopub.status.idle": "2025-09-18T12:51:44.922050Z",
     "shell.execute_reply": "2025-09-18T12:51:44.921162Z"
    }
   },
   "outputs": [],
   "source": [
    "df_report.to_csv(\"resources/you_first_data_quality_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Next?\n",
    "You might be thinking: \n",
    "this is great, but:\n",
    "* I don't want to have to write a separate function call for every rule I want to run\n",
    "* I don't want to manually add each rule to a Data Quality Config\n",
    "* I want to manage my regex patterns centrally, rather than write them out each time\n",
    "* I want to specify my rules in a text file so it's easier to change and manage\n",
    "    * and then run them all in one go\n",
    "* I want an easy way of generating that rules text file, based off of my Data Quality Report\n",
    "    * basically, 'please write out the rules that created this report'\n",
    "\n",
    "This will be covered in our Python 2 module :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchq-data-quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
